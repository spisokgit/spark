{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"transformation 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize(range(20), 4).map(lambda x: (x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partitionBy(numPartitions, partitionFunc=&lt;function portable_hash&gt;)\n",
    "\n",
    "Return a copy of the RDD partitioned using the specified partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
       " [(5, 5), (6, 6), (7, 7), (8, 8), (9, 9)],\n",
       " [(10, 10), (11, 11), (12, 12), (13, 13), (14, 14)],\n",
       " [(15, 15), (16, 16), (17, 17), (18, 18), (19, 19)]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (4, 4), (8, 8), (12, 12), (16, 16)],\n",
       " [(1, 1), (5, 5), (9, 9), (13, 13), (17, 17)],\n",
       " [(2, 2), (6, 6), (10, 10), (14, 14), (18, 18)],\n",
       " [(3, 3), (7, 7), (11, 11), (15, 15), (19, 19)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.partitionBy(4).glom().collect() # 해쉬함수에 의해 정해진대로 partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'{data.partitioner}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize(range(20), 4).map(lambda x: (x, x)).partitionBy(4)\n",
    "data2 = sc.parallelize(range(20), 4).map(lambda x: (x, x)).partitionBy(2)\n",
    "\n",
    "# partitioner가 다르면  narrow transformation안되고 wide가 된다\n",
    "data1.partitioner == data2.partitioner\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = sc.parallelize(range(20), 4).map(lambda x: (x, x)).partitionBy(4, lambda x: x*37)\n",
    "data1.partitioner == data3.partitioner\n",
    "# False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union(other)\n",
    "Return the union of this RDD and another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1], [2, 2, 3, 3, 4], [3, 3, 4], [4, 1, 1, 2]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize([1, 1, 1, 1, 2, 2, 3, 3, 4], 2)\n",
    "data2 = sc.parallelize([3, 3, 4, 4, 1, 1, 2], 2)\n",
    "\n",
    "# 중복제거하지 않음\n",
    "data1.union(data2).collect()\n",
    "# [1, 1, 1, 1, 2, 2, 3, 3, 4, 3, 3, 4, 4, 1, 1, 2]\n",
    "\n",
    "data1.union(data2).glom().collect()\n",
    "# [[1, 1, 1, 1], [2, 2, 3, 3, 4], [3, 3, 4], [4, 1, 1, 2]]\n",
    "# [[1, 1, 1, 1, 2, 2, 3, 3, 4], [3, 3, 4, 4, 1, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1], [2, 2, 3, 3, 4]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3, 4], [4, 1, 1, 2]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection(other)\n",
    "Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.\n",
    "\n",
    "**Note** This method performs a shuffle internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복제거\n",
    "data1 = sc.parallelize([1, 1, 1, 1, 2, 2, 3, 3, 4], 2)\n",
    "data2 = sc.parallelize([3, 3, 4, 4, 1, 1, 2], 2)\n",
    "data1.intersection(data2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4], [1], [2], [3]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파티션의 이동 shuffle 일어난다  # wide transformation\n",
    "data1.intersection(data2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cogroup(other, numPartitions=None)\n",
    "For each key k in self or other, return a resulting RDD that contains a tuple with the list of values for that key in self as well as other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(range(20),4).map(lambda x: (x, x))\n",
    "rdd2 = sc.parallelize(range(20),4).map(lambda x: (x, x))\n",
    "\n",
    "# groupByKey()는 동일 rdd내에서, cogroup은 다른 2개의 RDD의 key값을 중심으로 (key, tuple(lterable, lterable,,))\n",
    "#rdd1.cogroup(rdd2).collect()\n",
    "#     [(0,\n",
    "#       (<pyspark.resultiterable.ResultIterable at 0x7fe4d2b23a20>,\n",
    "#        <pyspark.resultiterable.ResultIterable at 0x7fe4d2b23128>)),\n",
    "#      (8,\n",
    "#       (<pyspark.resultiterable.ResultIterable at 0x7fe4d2b23518>,\n",
    "#        <pyspark.resultiterable.ResultIterable at 0x7fe4d2b239b0>)),,,,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([0], [0])),\n",
       " (8, ([8], [8])),\n",
       " (16, ([16], [16])),\n",
       " (1, ([1], [1])),\n",
       " (9, ([9], [9])),\n",
       " (17, ([17], [17])),\n",
       " (2, ([2], [2])),\n",
       " (10, ([10], [10])),\n",
       " (18, ([18], [18])),\n",
       " (3, ([3], [3])),\n",
       " (11, ([11], [11])),\n",
       " (19, ([19], [19])),\n",
       " (4, ([4], [4])),\n",
       " (12, ([12], [12])),\n",
       " (5, ([5], [5])),\n",
       " (13, ([13], [13])),\n",
       " (6, ([6], [6])),\n",
       " (14, ([14], [14])),\n",
       " (7, ([7], [7])),\n",
       " (15, ([15], [15]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.cogroup(rdd2).mapValues(lambda t: ( list(t[0]), list(t[1]) )).collect()\n",
    "# [(0, ([0], [0])),\n",
    "#  (8, ([8], [8])),\n",
    "#  (16, ([16], [16])),\n",
    "#  (1, ([1], [1])),\n",
    "#  (9, ([9], [9])),\n",
    "#  (17, ([17], [17])),\n",
    "#  (2, ([2], [2])),\n",
    "#  (10, ([10], [10])),\n",
    "#  (18, ([18], [18])),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
       " [(5, 5), (6, 6), (7, 7), (8, 8), (9, 9)],\n",
       " [(10, 10), (11, 11), (12, 12), (13, 13), (14, 14)],\n",
       " [(15, 15), (16, 16), (17, 17), (18, 18), (19, 19)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()\n",
    "# [[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
    "#  [(5, 5), (6, 6), (7, 7), (8, 8), (9, 9)],\n",
    "#  [(10, 10), (11, 11), (12, 12), (13, 13), (14, 14)],\n",
    "#  [(15, 15), (16, 16), (17, 17), (18, 18), (19, 19)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
       " [(5, 5), (6, 6), (7, 7), (8, 8), (9, 9)],\n",
       " [(10, 10), (11, 11), (12, 12), (13, 13), (14, 14)],\n",
       " [(15, 15), (16, 16), (17, 17), (18, 18), (19, 19)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.glom().collect()\n",
    "# [[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
    "#  [(5, 5), (6, 6), (7, 7), (8, 8), (9, 9)],\n",
    "#  [(10, 10), (11, 11), (12, 12), (13, 13), (14, 14)],\n",
    "#  [(15, 15), (16, 16), (17, 17), (18, 18), (19, 19)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, ([0], [0])), (8, ([8], [8])), (16, ([16], [16]))],\n",
       " [(1, ([1], [1])), (9, ([9], [9])), (17, ([17], [17]))],\n",
       " [(2, ([2], [2])), (10, ([10], [10])), (18, ([18], [18]))],\n",
       " [(3, ([3], [3])), (11, ([11], [11])), (19, ([19], [19]))],\n",
       " [(4, ([4], [4])), (12, ([12], [12]))],\n",
       " [(5, ([5], [5])), (13, ([13], [13]))],\n",
       " [(6, ([6], [6])), (14, ([14], [14]))],\n",
       " [(7, ([7], [7])), (15, ([15], [15]))]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide transformation # 파티션 개수가 늘어남 8개로 \n",
    "rdd1.cogroup(rdd2).mapValues(lambda t: ( list(t[0]), list(t[1]) )).glom().collect()\n",
    "# [[(0, ([0], [0])), (8, ([8], [8])), (16, ([16], [16]))],\n",
    "#  [(1, ([1], [1])), (9, ([9], [9])), (17, ([17], [17]))],\n",
    "#  [(2, ([2], [2])), (10, ([10], [10])), (18, ([18], [18]))],\n",
    "#  [(3, ([3], [3])), (11, ([11], [11])), (19, ([19], [19]))],\n",
    "#  [(4, ([4], [4])), (12, ([12], [12]))],\n",
    "#  [(5, ([5], [5])), (13, ([13], [13]))],\n",
    "#  [(6, ([6], [6])), (14, ([14], [14]))],\n",
    "#  [(7, ([7], [7])), (15, ([15], [15]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (4, 4), (8, 8), (12, 12), (16, 16)],\n",
       " [(1, 1), (5, 5), (9, 9), (13, 13), (17, 17)],\n",
       " [(2, 2), (6, 6), (10, 10), (14, 14), (18, 18)],\n",
       " [(3, 3), (7, 7), (11, 11), (15, 15), (19, 19)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=rdd1.partitionBy(4)\n",
    "rdd1.glom().collect()\n",
    "# [[(0, 0), (4, 4), (8, 8), (12, 12), (16, 16)],\n",
    "#  [(1, 1), (5, 5), (9, 9), (13, 13), (17, 17)],\n",
    "#  [(2, 2), (6, 6), (10, 10), (14, 14), (18, 18)],\n",
    "#  [(3, 3), (7, 7), (11, 11), (15, 15), (19, 19)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.rdd.Partitioner object at 0x7fe4d2a77e10>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'{rdd1.partitioner}') # <pyspark.rdd.Partitioner object at 0x7fe4d2a77e10> # 파티셔너가 있다\n",
    "print(f'{rdd2.partitioner}') # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, ([0], [0])),\n",
       "  (4, ([4], [4])),\n",
       "  (8, ([8], [8])),\n",
       "  (12, ([12], [12])),\n",
       "  (16, ([16], [16]))],\n",
       " [(1, ([1], [1])),\n",
       "  (5, ([5], [5])),\n",
       "  (9, ([9], [9])),\n",
       "  (13, ([13], [13])),\n",
       "  (17, ([17], [17]))],\n",
       " [(2, ([2], [2])),\n",
       "  (6, ([6], [6])),\n",
       "  (10, ([10], [10])),\n",
       "  (14, ([14], [14])),\n",
       "  (18, ([18], [18]))],\n",
       " [(3, ([3], [3])),\n",
       "  (7, ([7], [7])),\n",
       "  (11, ([11], [11])),\n",
       "  (15, ([15], [15])),\n",
       "  (19, ([19], [19]))]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd1.cogroup(rdd2, 4).mapValues(lambda t: ( list(t[0]), list(t[1]) ))\n",
    "rdd3.glom().collect() # rdd1은 파티셔너가 있어서 그대로 넘어오고, # rdd2는 shuffle이 일어나는 것을 알 수 있다\n",
    "# [[(0, ([0], [0])),\n",
    "#   (4, ([4], [4])),\n",
    "#   (8, ([8], [8])),\n",
    "#   (12, ([12], [12])),\n",
    "#   (16, ([16], [16]))],\n",
    "#  [(1, ([1], [1])),\n",
    "#   (5, ([5], [5])),\n",
    "#   (9, ([9], [9])),\n",
    "#   (13, ([13], [13])),\n",
    "#   (17, ([17], [17]))],\n",
    "#  [(2, ([2], [2])),\n",
    "#   (6, ([6], [6])),\n",
    "#   (10, ([10], [10])),\n",
    "#   (14, ([14], [14])),\n",
    "#   (18, ([18], [18]))],\n",
    "#  [(3, ([3], [3])),\n",
    "#   (7, ([7], [7])),\n",
    "#   (11, ([11], [11])),\n",
    "#   (15, ([15], [15])),\n",
    "#   (19, ([19], [19]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.partitioner == rdd3.partitioner\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0), (4, 4), (8, 8), (12, 12), (16, 16)],\n",
       " [(1, 1), (5, 5), (9, 9), (13, 13), (17, 17)],\n",
       " [(2, 2), (6, 6), (10, 10), (14, 14), (18, 18)],\n",
       " [(3, 3), (7, 7), (11, 11), (15, 15), (19, 19)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### rdd2의 파티셔너 같게 하자 partitionBy 제공하여 \n",
    "rdd2 = rdd2.partitionBy(4)\n",
    "rdd2.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f'{rdd1.partitioner == rdd2.partitioner}') # True\n",
    "print(f'{rdd2.partitioner == rdd3.partitioner}') # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, ([0], [0])),\n",
       "  (4, ([4], [4])),\n",
       "  (8, ([8], [8])),\n",
       "  (12, ([12], [12])),\n",
       "  (16, ([16], [16]))],\n",
       " [(1, ([1], [1])),\n",
       "  (5, ([5], [5])),\n",
       "  (9, ([9], [9])),\n",
       "  (13, ([13], [13])),\n",
       "  (17, ([17], [17]))],\n",
       " [(2, ([2], [2])),\n",
       "  (6, ([6], [6])),\n",
       "  (10, ([10], [10])),\n",
       "  (14, ([14], [14])),\n",
       "  (18, ([18], [18]))],\n",
       " [(3, ([3], [3])),\n",
       "  (7, ([7], [7])),\n",
       "  (11, ([11], [11])),\n",
       "  (15, ([15], [15])),\n",
       "  (19, ([19], [19]))]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow transformation\n",
    "rdd4=rdd1.cogroup(rdd2, 4).mapValues(lambda t: ( list(t[0]), list(t[1]) ))\n",
    "rdd4.glom().collect()\n",
    "# [[(0, ([0], [0])),\n",
    "#   (4, ([4], [4])),\n",
    "#   (8, ([8], [8])),\n",
    "#   (12, ([12], [12])),\n",
    "#   (16, ([16], [16]))],\n",
    "#  [(1, ([1], [1])),\n",
    "#   (5, ([5], [5])),\n",
    "#   (9, ([9], [9])),\n",
    "#   (13, ([13], [13])),\n",
    "#   (17, ([17], [17]))],\n",
    "#  [(2, ([2], [2])),\n",
    "#   (6, ([6], [6])),\n",
    "#   (10, ([10], [10])),\n",
    "#   (14, ([14], [14])),\n",
    "#   (18, ([18], [18]))],\n",
    "#  [(3, ([3], [3])),\n",
    "#   (7, ([7], [7])),\n",
    "#   (11, ([11], [11])),\n",
    "#   (15, ([15], [15])),\n",
    "#   (19, ([19], [19]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join(other, numPartitions=None)\n",
    "Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "\n",
    "Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.\n",
    "\n",
    "Performs a hash join across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (2, 2)), (2, (2, 2)), (2, (2, 2)), (2, (2, 2)), (1, (1, 1)), (1, (1, 1))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,1,2,2,]).map(lambda x: (x,x))\n",
    "rdd2 = sc.parallelize([1,4,2,2,]).map(lambda x: (x,x))\n",
    "rdd1.join(rdd2).collect()\n",
    "# [(2, (2, 2)), (2, (2, 2)), (2, (2, 2)), (2, (2, 2)), (1, (1, 1)), (1, (1, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
