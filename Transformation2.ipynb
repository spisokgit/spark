{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"transformation 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy(f, numPartitions=None, partitionFunc=\"function portable_hash\")\n",
    "\n",
    "Return an RDD of grouped items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, <pyspark.resultiterable.ResultIterable object at 0x7ff1d2b332b0>), (2, <pyspark.resultiterable.ResultIterable object at 0x7ff1d2b33320>), (0, <pyspark.resultiterable.ResultIterable object at 0x7ff1d2b33390>)]\n"
     ]
    }
   ],
   "source": [
    "# pandas groupby 처럼 aggration함수가 필요\n",
    "# groupby(함수) 함수 결과값을 key로 하는 object 생성\n",
    "data = sc.parallelize ([1, 1, 2, 3, 4, 5, 5, 6, 6, 6])\n",
    "result = data.groupBy(lambda x: x%3)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, [1, 1, 4]), (2, [2, 5, 5]), (0, [3, 6, 6, 6])]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize ([1, 1, 2, 3, 4, 5, 5, 6, 6, 6])\n",
    "result = data.groupBy(lambda x: x%3).mapValues(list).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey(numPartitions=None, partitionFunc=\"function portable_hash\")\n",
    "\n",
    "Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "\n",
    "Note If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 3]), ('b', [2]), ('c', [3])]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 3)])\n",
    "print(data.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 3)])\n",
    "print(data.groupByKey().mapValues(len).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey vs. ReduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 2), (0, 3), (1, 4), (2, 5), (0, 6), (1, 7), (2, 8), (0, 9)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 18), (1, 12), (2, 15)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey network cost가 적다\n",
    "data = sc.parallelize(range(10)).map(lambda a: (a%3, a))\n",
    "print(data.collect())\n",
    "data.reduceByKey(lambda a, b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 18), (1, 12), (2, 15)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey()결과는 리스트[0] key, 리스트[1] value 리턴된다고 생각\n",
    "# network cost가 크다\n",
    "data.groupByKey().map(lambda t: (t[0], sum(t[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey(ascending=True, numPartitions=None, keyfunc=\"lambda function\")\n",
    "\n",
    "Sorts this RDD, which is assumed to consist of (key, value) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(\"b\", 1), (\"a\", 1), (\"c\", 1), (\"a\", 1)])\n",
    "data.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 1), ('b', 1), ('a', 1), ('a', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(\"b\", 1), (\"a\", 1), (\"c\", 1), (\"a\", 1)])\n",
    "data.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortBy(keyfunc, ascending=True, numPartitions=None)\n",
    "Sorts this RDD by the given keyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 3, 4, 4, 4, 5, 7, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy(함수)  함수결과값으로 하는 sort\n",
    "data = sc.parallelize([4, 3, 2, 4, 2, 7, 9, 4, 5, 2])\n",
    "data.sortBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7, 5, 4, 4, 4, 3, 2, 2, 2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy(함수)  함수결과값으로 하는 sort\n",
    "data = sc.parallelize([4, 3, 2, 4, 2, 7, 9, 4, 5, 2])\n",
    "data.sortBy(lambda x: x, False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce(numPartitions, shuffle=False)\n",
    "Return a new RDD that is reduced into numPartitions partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5, 6], [7, 8, 9, 10], [11, 12, 13, 14]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(15), 4)\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 13, 14]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(15), 4)\n",
    "data.coalesce(3).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5, 6], [7, 8, 9, 10], [11, 12, 13, 14]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle 없이 파티션을 줄이는 함수 # 증가시키지는 못함\n",
    "data = sc.parallelize(range(15), 4)\n",
    "data.coalesce(10).glom().collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition(numPartitions)\n",
    "Return a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "Can increase or decrease the level of parallelism in this RDD. Internally, this uses a shuffle to redistribute data. ***If you are decreasing the number of partitions in this RDD, consider using coalesce, which can avoid performing a shuffle.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 7, 8, 9, 10, 11, 12, 13, 14], [3, 4, 5, 6]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.repartition(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [0, 1, 2, 7, 8, 9, 10], [3, 4, 5, 6, 11, 12, 13, 14]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해쉬함수에 의해 분배\n",
    "data.repartition(3).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample(withReplacement, fraction, seed=None)\n",
    "Return a sampled subset of this RDD.\n",
    "\n",
    "Parameters\n",
    "* withReplacement – can elements be sampled multiple times (replaced when sampled out)\n",
    "* fraction – expected size of the sample as a fraction of this RDD’s size without replacement: probability that each element is chosen; fraction must be \\[0, 1\\] with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "* seed – seed for the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 2, 2, 3, 5, 12, 13, 14]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복추출여부, fraction expeted size 기대값이므로 절대적으로 50%를 가져오지 않음, seed\n",
    "data.sample(True, 0.5, 13).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct(numPartitions=None)\n",
    "Return a new RDD containing the distinct elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([1, 1, 2, 3, 4, 5, 6, 2, 3, 5])\n",
    "data.distinct().collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
