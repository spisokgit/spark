{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local\", appName=\"first app\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140691153533384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action - collect()\n",
    "rdd = sc.parallelize(range(1,11))\n",
    "result = rdd.collect()\n",
    "print(result) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "id(result)    #139931150042440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10914784"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action - count()\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "result = rdd.count()\n",
    "print(result)  # 10\n",
    "id(result)     # 10914784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# transformation Action - map, collect()\n",
    "rdd1 = sc.parallelize(range(1, 6))\n",
    "rdd2= rdd1.map(lambda v : v+1)\n",
    "print(rdd2.collect())\n",
    "# [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'orange'], ['grape', 'apple', 'mango'], ['blueberry', 'tomato', 'orange']]\n"
     ]
    }
   ],
   "source": [
    "# map \n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.map(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [['apple', 'orange'], ['grape', 'apple', 'mango'], ['blueberry', 'tomato', 'orange']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'orange', 'grape', 'apple', 'mango', 'blueberry', 'tomato', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.flatMap(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# ['apple', 'orange', 'grape', 'apple', 'mango', 'blueberry', 'tomato', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[275] at RDD at PythonRDD.scala:53\n",
      "[[], None, [], None, [], None]\n"
     ]
    }
   ],
   "source": [
    "# python코드 없이 만들어봄  # page 111\n",
    "# flatMap return type TraversableOnce[U]   # None 리턴되는 것이 중요\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple.mango\", \"blueberry,tomato,orange\"])\n",
    "def deflog(log):\n",
    "    print(str(log)) # 실행되지 않음\n",
    "    if \"apple\" == str(log):\n",
    "        return list[log]\n",
    "    else:\n",
    "        return list(), print(type(log))\n",
    "# deflog = [ if \"apple\" == str(log) ]\n",
    "rdd2 = rdd1.flatMap(deflog)  # transformation에서는 error발생치 않음\n",
    "print(rdd2)                  # PythonRDD[12] at RDD at PythonRDD.scala:53\n",
    "print(rdd2.collect())       #  [[], None, [], None, [], None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions\n",
    "# map()과 flatMap() RDD의 각 요소를 하나씩 처리\n",
    "# mapPartition() 파티션단위로 처리:파티션에 속한 모든 요소의 컬렉션에 대한 이터레이터(Iterator)를 입력으로 사용,리턴도 이터레이터\n",
    "# 파티션 단위의 중간산출물을 만들거나 DB 연결과 같은 고비용의 자원을 파티션 단위로 공유해 사용할 수 있다는 장점\n",
    "\n",
    "# increase\n",
    "def increase(numbers):\n",
    "    print(\"DB 연결 !!!\")\n",
    "    return(i + 1 for i in numbers)\n",
    "\n",
    "#rdd1 = sc.parallelize(range(1,11))   # 옵션으로 partition 정할 수 있다\n",
    "rdd1 = sc.parallelize(range(1,11), 3)   # 옵션으로 partition 정할 수 있다\n",
    "rdd2 = rdd1.mapPartitions(increase)    # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "#[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "# 파티션에 속한 요소의 정보 + 해당파티션의 인덱스 함께 전달\n",
    "#increaseWithIndex\n",
    "def increaseWithIndex(idx, numbers):\n",
    "    print(\"partitions !!!\")\n",
    "    for i in numbers:\n",
    "        if(idx == 1):\n",
    "            yield i + 1  # yield 일시정지 변수기억 next()\n",
    "\n",
    "            \n",
    "rdd1 = sc.parallelize(range(1,11), 3)\n",
    "rdd2 = rdd1.mapPartitionsWithIndex(increaseWithIndex)  # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "# [5, 6, 7]  <== [ 4, 5, 6] 파티션이 index 1로 전달되었음을 추정할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 2), ('c', 2)]\n"
     ]
    }
   ],
   "source": [
    "# mapValues  # RDD의 요소가 key value의 쌍으로 이루는 경우 페어RDD(PairRDD)\n",
    "# 인자로 받은 value에 해당하는 요소에만 적용, 그 결과로 구성된 새로운 RDD 생성\n",
    "\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.map(lambda v : (v, 1))\n",
    "\n",
    "rdd3 = rdd2.mapValues(lambda i: i + 1)\n",
    "print(rdd3.collect())\n",
    "# [('a', 2), ('b', 2), ('c', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'c'), (1, 'd'), (1, 'e')]\n"
     ]
    }
   ],
   "source": [
    "# flatMapValues\n",
    "\n",
    "rdd1 = sc.parallelize([(1, \"a,b\"), (2, \"a,c\"), (1, \"d,e\")])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.flatMapValues(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'c'), (1, 'd'), (1, 'e')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그룹과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.api.java.JavaPairRDD@4e0d2262\n",
      "[('a', 1), ('b', 2), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "# zip  # key value # 요소의 개수 같아야 한다\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "result = rdd1.zip(rdd2)\n",
    "print(result) #org.apache.spark.api.java.JavaPairRDD@37a7333b\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipPartitions  파티션단위로 zip()연산 수행 # 파티션 개수 같아야 한다\n",
    "# 파이썬에서는 사용할 수 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[28] at RDD at PythonRDD.scala:53\n",
      "[('odd', <pyspark.resultiterable.ResultIterable object at 0x7ff5363a9b00>), ('even', <pyspark.resultiterable.ResultIterable object at 0x7ff5363a9ba8>)]\n",
      "odd [1, 3, 5, 7, 9]\n",
      "even [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# groupBy # value값을 새로운 key값으로 하는 (key, value그룹(시퀸스))으로 생성\n",
    "# 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할 담당\n",
    "\n",
    "rdd1 = sc.parallelize(range(1, 11))\n",
    "rdd2 = rdd1.groupBy(lambda v: \"even\" if v%2==0 else \"odd\")\n",
    "print(rdd2) # PythonRDD[38] at RDD at PythonRDD.scala:53\n",
    "print(rdd2.collect())\n",
    "#[('odd', <pyspark.resultiterable.ResultIterable object at 0x7f444277e860>), ('even', <pyspark.resultiterable.ResultIterable object at 0x7f444277e828>)]\n",
    "for x in rdd2.collect():\n",
    "    #print(x[0], x[1])\n",
    "# odd <pyspark.resultiterable.ResultIterable object at 0x7f44432759e8>\n",
    "# even <pyspark.resultiterable.ResultIterable object at 0x7f44604d7160>\n",
    "    print(x[0], list(x[1]))\n",
    "# odd [1, 3, 5, 7, 9]\n",
    "# even [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1]\n",
      "b [1, 1]\n",
      "c [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey  # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 같은 키를 가진 요소들로 그룹을 만들고\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"]).map(lambda v: (v, 1))\n",
    "rdd2 = rdd1.groupByKey()\n",
    "for x in rdd2.collect():\n",
    "    print(x[0], list(x[1]))\n",
    "# a [1]\n",
    "# b [1, 1]\n",
    "# c [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[44] at RDD at PythonRDD.scala:53\n",
      "[('k2', (<pyspark.resultiterable.ResultIterable object at 0x7ff53636dba8>, <pyspark.resultiterable.ResultIterable object at 0x7ff53636dc88>)), ('k1', (<pyspark.resultiterable.ResultIterable object at 0x7ff53636db70>, <pyspark.resultiterable.ResultIterable object at 0x7ff53636dda0>))]\n",
      "k2 ['v2'] []\n",
      "k1 ['v1', 'v3'] ['v4']\n"
     ]
    }
   ],
   "source": [
    "# cogroup   ## RDD구성요소 key, value에서만 가능\n",
    "# join과 비교\n",
    "# 여러 RDD에서 같은 키를 갖는 값 요소를 찾아서 키와 그 키에 속하는 요소의 시퀀스(List, Vector 등의 상위클래스인 Iterable)\n",
    "# 구성된 튜플, 그 튜플로 구성된 RDD 생성 \n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k1\", \"v3\")])\n",
    "rdd2 = sc.parallelize([(\"k1\", \"v4\")])\n",
    "result = rdd1.cogroup(rdd2)\n",
    "print(result) # PythonRDD[120] at RDD at PythonRDD.scala:53\n",
    "print(result.collect())\n",
    "# [('k2', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7e48>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be7c50>)), ('k1', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7dd8>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be4f28>))]\n",
    "for x in result.collect():\n",
    "    print(x[0], list(x[1][0]), list(x[1][1]))\n",
    "# k2 ['v2'] []\n",
    "# k1 ['v1', 'v3'] ['v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집합과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# distinct  # RDD의 원소에서 중복을 제외한 요소로만 구성된 새로운 RDD 생성\n",
    "rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])\n",
    "result = rdd.distinct()\n",
    "print(result.collect())\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "result = rdd1.cartesian(rdd2)\n",
    "print(result.collect())\n",
    "#[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'a']\n"
     ]
    }
   ],
   "source": [
    "# subtract  # 두 개의 RDD가 있을 때 rdd1- rdd2\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\"])\n",
    "result = rdd1.subtract(rdd2)\n",
    "print(result.collect())\n",
    "#['b', 'c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "# union \n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\", \"f\"])\n",
    "result = rdd1.union(rdd2)\n",
    "print(result.collect())\n",
    "#['a', 'b', 'c', 'd', 'e', 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'a']\n"
     ]
    }
   ],
   "source": [
    "# intersection  #  중복제외됨\n",
    "rdd1 = sc.parallelize ([\"a\", \"a\", \"b\", \"c\" ])\n",
    "rdd2 = sc.parallelize ([\"a\", \"a\", \"c\", \"c\"])\n",
    "result = rdd1.intersection(rdd2)\n",
    "print(result.collect())\n",
    "# ['c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (1, 2)), ('c', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# join  # RDD구성요소 key, value에서만 가능\n",
    "# cogroup와 비교  # Action값을 바로 볼 수 있다\n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result = rdd1.join(rdd2)\n",
    "print(result.collect())\n",
    "#[('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leftOuterJoin: [('b', (1, 2)), ('c', (1, 2)), ('d', (1, None)), ('a', (1, None)), ('e', (1, None))]\n",
      "rightOuterJoin: [('b', (1, 2)), ('c', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# leftOuterJoin, rightOuterJoin  # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 외부조인을 수행\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result1 = rdd1.leftOuterJoin(rdd2)\n",
    "result2 = rdd1.rightOuterJoin(rdd2)\n",
    "print(f'leftOuterJoin: {result1.collect()}')\n",
    "print(f'rightOuterJoin: {result2.collect()}')\n",
    "# leftOuterJoin: [('b', (1, 2)), ('c', (1, 2)), ('d', (1, None)), ('a', (1, None)), ('e', (1, None))]\n",
    "# rightOuterJoin: [('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# subtractByKey  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\"]).map(lambda v: (v, 1))\n",
    "result = rdd1.subtractByKey(rdd2)\n",
    "print(result.collect())\n",
    "# [('a', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 집계와 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey  # RDD구성요소 key, value에서만 가능\n",
    "# 2개의 값을 하나로 합치는 함수를 인자로 전달받는데, 이 함수는 결합, 교환법칙 성립\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# foldByKey   # RDD구성요소 key, value에서만 가능\n",
    "# reduceByKey와는 달리 병합연산의 초기값을 메서드의 인자로 전달해서 병합시 사용\n",
    "# ex) 더하는 함수 0, 두 문자열 연결 공백문자\"\"를 초기값으로 사용가능\n",
    "# 하지만 이 때 초기값이 반복해도 연산결과에 영향을 주지 않는 값이여야 함\n",
    "# 함수는 교환법칙은 만족안해도 되고 결합법칙은 만족해야 함\n",
    "\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.foldByKey(0, lambda v1,v2:v1+v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 58, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-517ea8fca3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Math\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Math\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreateCombiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmergeValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmergeCombiners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Math'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Math'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Eng'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m#print(str(result.collectAsMap()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \"\"\"\n\u001b[0;32m-> 1587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 58, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# error  # page 137\n",
    "# combineByKey    # RDD구성요소 key, value에서만 가능\n",
    "class Record:\n",
    "        \n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "        \n",
    "# combineBy\n",
    "def createCombiner(v):\n",
    "    return Record(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeValue(c, v):\n",
    "    return c.addAmt(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeCombiners(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "rdd = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n",
    "result = rdd.combineByKey(lambda v: createCombiner(v), lambda c, v: mergeValue(c, v), lambda c1, c2: mergeCombiners(c1, c2))\n",
    "print('Math', result.collectAsMap()['Math'], 'Eng', result.collectAsMap()['Eng'])\n",
    "#print(str(result.collectAsMap()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateByKey   # RDD구성요소 key, value에서만 가능\n",
    "# combineByKey()와 동일하나 초기값 생성하는 부분만 다름\n",
    "# result = rdd.aggregateByKey(zero)(mergeValue, mergecombiners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipe 및 파티션과 관련된 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,3', '4,6', '7,9']\n"
     ]
    }
   ],
   "source": [
    "# pipe\n",
    "rdd = sc.parallelize( [\"1,2,3\", \"4,5,6\", \"7,8,9\"])\n",
    "result = rdd.pipe(\"cut -f 1,3 -d ,\") # -field  1,3출력 구분자 -d=delimeter default tab, 여기서는 ,\n",
    "print(result.collect())\n",
    "# ['1,3', '4,6', '7,9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition size: 10\n",
      "partition size: 5\n",
      "partition size: 10\n"
     ]
    }
   ],
   "source": [
    "# 최초 설정한 partition 수 조정필요시\n",
    "# coalesce     : 줄이기만 가능         셔플 수행하라는 옵션지정하지 않는  셔플을 사용하지 않기 때문\n",
    "# repartition : 늘리거나 줄일 수 있음  셔플 기반으로 동작\n",
    "rdd1= sc.parallelize(list(range(1, 11)),10)\n",
    "rdd2 = rdd1.coalesce(5)\n",
    "rdd3 = rdd2.repartition(10)\n",
    "print(f'partition size: {rdd1.getNumPartitions()}')\n",
    "print(f'partition size: {rdd2.getNumPartitions()}')\n",
    "print(f'partition size: {rdd3.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 142\n",
    "# repartitionAndSortWithinPartitions    # RDD구성요소 key, value에서만 가능\n",
    "# RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리, 각 파티션단위로 정렬수행한 뒤, \n",
    "# 그 결과로 새로운  RDD을  생성\n",
    "# 메서드 실행시 각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너(Partitioner)를 설정\n",
    "# 파티셔너는 각 데이터의 키 값을 이용해 데이터가 속할 파티션 결정, 키 값을 이용한 정렬도 함께 수행\n",
    "# 즉 파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬 따로하는 것보다 높은 성능\n",
    "\n",
    "import random\n",
    "data = [random.randrange(1,100) for i in range(0, 10)]\n",
    "#print(data) # [23, 99, 61, 18, 84, 58, 75, 13, 55, 47]\n",
    "\n",
    "rdd1 = sc.parallelize(data).map(lambda v: (v, \"-\"))\n",
    "#print(rdd1.collect())  # [(73, '-'), (79, '-'), (47, '-'), (98, '-'), (33, '-'), (51, '-'), (43, '-'), (68, '-'), (95, '-'), (69, '-')]\n",
    "\n",
    "rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x: x) # HashPartitioner 파티셔너 3\n",
    "# print(rdd2) # PythonRDD[187] at RDD at PythonRDD.scala:53\n",
    "# rdd2.count # <bound method RDD.count of PythonRDD[315] at RDD at PythonRDD.scala:53>\n",
    "\n",
    "# 아래코드 에러는 없으나 결과 출력물이 안나옴  # cli에서는 나옴 (foreach관련) # foreach(함수)\n",
    "# rdd2.foreachPartition(lambda values: print(list(values))) # RDD의 파티션단위로 특정함수를 실행\n",
    "# [Stage 0:> (0 + 1) / 1][(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "rdd2.foreachPartition(  lambda values: print( list( map(lambda v: v, values)) ) ) \n",
    "# [(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "\n",
    "#아래코드 에러는 없으나 결과 출력물이 안나옴   # cli 에서는 나옴 (foreach)\n",
    "# def test(it):\n",
    "#     print(\"=\"*5)\n",
    "#     print ( list( map(lambda x: x, it) ) )\n",
    "# rdd2.foreachPartition(test) \n",
    "# =====\n",
    "# [(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# =====\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# =====\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "\n",
    "# 아래코드는 Py4JJavaErro # cli에서도 error # foreach 2번\n",
    "# rdd2.foreachPartition( lambda it: it.foreach(lambda v: print(v))  ) # RDD의 파티션단위로 특정함수를 실행\n",
    "\n",
    "# 아래코드는 Py4JJavaErro # error\n",
    "# def test(it):\n",
    "#     print(\"=\"*5)\n",
    "#     it.foreach( lambda x: print(x) )             \n",
    "# rdd2.foreachPartition(test)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1: 5 \n",
      "rdd2: 3 \n"
     ]
    }
   ],
   "source": [
    "# partitionBy   # RDD구성요소 key, value에서만 가능 \n",
    "# 파티셔너(Partitioner) 인자로 전달\n",
    "# HashPartitioner   : 각 요소의 키 값으로부터 해시값을 취해 이 값을 기준으로 파티션을 결정하는 방법\n",
    "# Rangepartitioner : 순서가 있는 요소들(Sortable)로 구성된 RDD에 사용가능,\n",
    "#                    각 요소를 목표 파티션 크기에 맞게 일정크기의 구간으로 나누는 방식\n",
    "\n",
    "rdd1 = sc.parallelize( [ (\"apple\", 1), (\"mouse\", 1), (\"monitor\", 1)],  5 )\n",
    "rdd2 = rdd1.partitionBy(3)\n",
    "print(f'rdd1: {rdd1.getNumPartitions()} ')  # rdd1: 5 \n",
    "print(f'rdd2: {rdd2.getNumPartitions()} ')  # rdd2: 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list ( map (함수, iter) ) 연습\n",
    "\n",
    "# li = [1, 2, 3]\n",
    "# result = map(lambda i: i * i, li)\n",
    "# print(next(result))  # 1\n",
    "# print(next(result))  # 1\n",
    "# print( list(map(lambda i: i * i, li)) )  #[1, 4, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필터와 정렬 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# filter   # python filter # list( filter(함수,iter) )\n",
    "rdd1 = sc.parallelize(range(1,6))\n",
    "rdd2 = rdd1.filter(lambda i : i > 2)\n",
    "print(rdd2.collect())\n",
    "# [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('q', 1), ('z', 1)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey    # RDD구성요소 key, value에서만 가능\n",
    "# 키 값을 기준으로 요소를 정렬하는 연산  # 소팅 완료 후 파티션 내부의 요소는 소팅 순서상 인접한요소로 재구성\n",
    "rdd = sc.parallelize( [(\"q\",1), (\"z\", 1), (\"a\", 1)])\n",
    "result = rdd.sortByKey()\n",
    "print(result.collect())\n",
    "# [('a', 1), ('q', 1), ('z', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k1', 'k2', ' k3']\n",
      "['v1', 'v2', 'v3']\n"
     ]
    }
   ],
   "source": [
    "# keys()  키에 해당하는 요소로 구성된 RDD 생성\n",
    "# values() 값에 해당하는 요소로 구성된 RDD 생성\n",
    "rdd = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\" k3\", \"v3\")])\n",
    "result1 = rdd.keys()\n",
    "result2 = rdd.values()\n",
    "print(result1.collect()) # ['k1', 'k2', ' k3']\n",
    "print(result2.collect()) # ['v1', 'v2', 'v3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "# 스칼라 API    sample(withReplacement: Boolean, fraction: Double, seed: Long=Utils.nextLong): RDD[t]\n",
    "# withReplacement : 복원추출 수행여부 True, False \n",
    "# fraction 복원일경우 : 샘플내에서 각 요소가 나타나는 횟수에 대한 기대값, 즉 각요소의 평균발생 횟수, 0이상\n",
    "#          비복원일경우:          각 요소가 샘플에 포함될 확률, 0 ~ 1 사이 값으로 지정가능\n",
    "\n",
    "# 샘플의 크기를 정해놓고 추출을 실행하지 않음  fraction 0.5 전체크기 50% 샘플추출아님\n",
    "# 크기를 정해놓은 메소드 takeSample()\n",
    "# seed : 일반적인 무작위 값 추출시 사용하는 것과 유사한 개념으로 반복시행시 결과가 바뀌지 않도록 제어 목적\n",
    "\n",
    "rdd = sc.parallelize(range(1, 101))\n",
    "result1 = rdd.sample(False, 0.5, 666) \n",
    "result2 = rdd.sample(True, 1.5, 777)\n",
    "print(f'result1.count(): {result1.count()}  result1.take(5): {result1.take(5)}') \n",
    "# result1.count(): 54  result1.take(5): [2, 4, 5, 6, 7]\n",
    "print(f'result2.count(): {result2.count()}  result2.take(5): {result2.take(5)}') \n",
    "# result2.count(): 153  result2.take(5): [1, 4, 4, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
