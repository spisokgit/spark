{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local\", appName=\"first app\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맛보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action - collect()\n",
    "rdd = sc.parallelize(range(1,11))\n",
    "result = rdd.collect()\n",
    "print(result) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "id(result)    #139931150042440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action - count()\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "result = rdd.count()\n",
    "print(result)  # 10\n",
    "id(result)     # 10914784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "## Map과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation Action - map, collect()\n",
    "rdd1 = sc.parallelize(range(1, 6))\n",
    "rdd2= rdd1.map(lambda v : v+1)\n",
    "print(rdd2.collect())\n",
    "# [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map()\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.map(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [['apple', 'orange'], ['grape', 'apple', 'mango'], ['blueberry', 'tomato', 'orange']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap()\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.flatMap(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# ['apple', 'orange', 'grape', 'apple', 'mango', 'blueberry', 'tomato', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python코드 없이 만들어봄  # page 111\n",
    "# flatMap return type TraversableOnce[U]   # None 리턴되는 것이 중요\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple.mango\", \"blueberry,tomato,orange\"])\n",
    "def deflog(log):\n",
    "    print(str(log)) # 실행되지 않음\n",
    "    if \"apple\" == str(log):\n",
    "        return list[log]\n",
    "    else:\n",
    "        return list(), print(type(log))\n",
    "# deflog = [ if \"apple\" == str(log) ]\n",
    "rdd2 = rdd1.flatMap(deflog)  # transformation에서는 error발생치 않음\n",
    "print(rdd2)                  # PythonRDD[12] at RDD at PythonRDD.scala:53\n",
    "print(rdd2.collect())       #  [[], None, [], None, [], None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitions()\n",
    "# map()과 flatMap() RDD의 각 요소를 하나씩 처리\n",
    "# mapPartition() 파티션단위로 처리:파티션에 속한 모든 요소의 컬렉션에 대한 이터레이터(Iterator)를 입력으로 사용,리턴도 이터레이터\n",
    "# 파티션 단위의 중간산출물을 만들거나 DB 연결과 같은 고비용의 자원을 파티션 단위로 공유해 사용할 수 있다는 장점\n",
    "\n",
    "# increase\n",
    "def increase(numbers):\n",
    "    print(\"DB 연결 !!!\")\n",
    "    return(i + 1 for i in numbers)\n",
    "\n",
    "#rdd1 = sc.parallelize(range(1,11))   # 옵션으로 partition 정할 수 있다\n",
    "rdd1 = sc.parallelize(range(1,11), 3)   # 옵션으로 partition 정할 수 있다\n",
    "rdd2 = rdd1.mapPartitions(increase)    # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "#[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitionsWithIndex()\n",
    "# 파티션에 속한 요소의 정보 + 해당파티션의 인덱스 함께 전달\n",
    "#increaseWithIndex\n",
    "def increaseWithIndex(idx, numbers):\n",
    "    print(\"partitions !!!\")\n",
    "    for i in numbers:\n",
    "        if(idx == 1):\n",
    "            yield i + 1  # yield 일시정지 변수기억 next()\n",
    "\n",
    "            \n",
    "rdd1 = sc.parallelize(range(1,11), 3)\n",
    "rdd2 = rdd1.mapPartitionsWithIndex(increaseWithIndex)  # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "# [5, 6, 7]  <== [ 4, 5, 6] 파티션이 index 1로 전달되었음을 추정할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapValue()  # RDD의 요소가 key value의 쌍으로 이루는 경우 페어RDD(PairRDD)\n",
    "# 인자로 받은 value에 해당하는 요소에만 적용, 그 결과로 구성된 새로운 RDD 생성\n",
    "\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.map(lambda v : (v, 1))\n",
    "\n",
    "rdd3 = rdd2.mapValues(lambda i: i + 1)\n",
    "print(rdd3.collect())\n",
    "# [('a', 2), ('b', 2), ('c', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMapValues()\n",
    "\n",
    "rdd1 = sc.parallelize([(1, \"a,b\"), (2, \"a,c\"), (1, \"d,e\")])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.flatMapValues(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'c'), (1, 'd'), (1, 'e')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그룹과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip()    # key value 생성 # 요소의 개수 같아야 한다\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "result = rdd1.zip(rdd2)\n",
    "print(result) #org.apache.spark.api.java.JavaPairRDD@37a7333b\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipPartitions  파티션단위로 zip()연산 수행 # 파티션 개수 같아야 한다\n",
    "# 파이썬에서는 사용할 수 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBy()   # value값을 새로운 key값으로 하는 (key, value그룹(시퀸스))으로 생성\n",
    "# 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할 담당\n",
    "\n",
    "rdd1 = sc.parallelize(range(1, 11))\n",
    "rdd2 = rdd1.groupBy(lambda v: \"even\" if v%2==0 else \"odd\")\n",
    "print(rdd2) # PythonRDD[38] at RDD at PythonRDD.scala:53\n",
    "print(rdd2.collect())\n",
    "#[('odd', <pyspark.resultiterable.ResultIterable object at 0x7f444277e860>), ('even', <pyspark.resultiterable.ResultIterable object at 0x7f444277e828>)]\n",
    "for x in rdd2.collect():\n",
    "    #print(x[0], x[1])\n",
    "# odd <pyspark.resultiterable.ResultIterable object at 0x7f44432759e8>\n",
    "# even <pyspark.resultiterable.ResultIterable object at 0x7f44604d7160>\n",
    "    print(x[0], list(x[1]))\n",
    "# odd [1, 3, 5, 7, 9]\n",
    "# even [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey()  # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 같은 키를 가진 요소들로 그룹을 만들고\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"]).map(lambda v: (v, 1))\n",
    "rdd2 = rdd1.groupByKey()\n",
    "for x in rdd2.collect():\n",
    "    print(x[0], list(x[1]))\n",
    "# a [1]\n",
    "# b [1, 1]\n",
    "# c [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogroup()   ## RDD구성요소 key, value에서만 가능\n",
    "# join과 비교\n",
    "# 여러 RDD에서 같은 키를 갖는 값 요소를 찾아서 키와 그 키에 속하는 요소의 시퀀스(List, Vector 등의 상위클래스인 Iterable)\n",
    "# 구성된 튜플, 그 튜플로 구성된 RDD 생성 \n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k1\", \"v3\")])\n",
    "rdd2 = sc.parallelize([(\"k1\", \"v4\")])\n",
    "result = rdd1.cogroup(rdd2)\n",
    "print(result) # PythonRDD[120] at RDD at PythonRDD.scala:53\n",
    "print(result.collect())\n",
    "# [('k2', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7e48>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be7c50>)), ('k1', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7dd8>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be4f28>))]\n",
    "for x in result.collect():\n",
    "    print(x[0], list(x[1][0]), list(x[1][1]))\n",
    "# k2 ['v2'] []\n",
    "# k1 ['v1', 'v3'] ['v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집합과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct()  # RDD의 원소에서 중복을 제외한 요소로만 구성된 새로운 RDD 생성\n",
    "rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])\n",
    "result = rdd.distinct()\n",
    "print(result.collect())\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartesian()  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "result = rdd1.cartesian(rdd2)\n",
    "print(result.collect())\n",
    "#[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract()  # 두 개의 RDD가 있을 때 rdd1- rdd2\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\"])\n",
    "result = rdd1.subtract(rdd2)\n",
    "print(result.collect())\n",
    "#['b', 'c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union()\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\", \"f\"])\n",
    "result = rdd1.union(rdd2)\n",
    "print(result.collect())\n",
    "#['a', 'b', 'c', 'd', 'e', 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection()  #  중복제외됨\n",
    "rdd1 = sc.parallelize ([\"a\", \"a\", \"b\", \"c\" ])\n",
    "rdd2 = sc.parallelize ([\"a\", \"a\", \"c\", \"c\"])\n",
    "result = rdd1.intersection(rdd2)\n",
    "print(result.collect())\n",
    "# ['c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join()  # RDD구성요소 key, value에서만 가능\n",
    "# cogroup와 비교  # Action값을 바로 볼 수 있다\n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result = rdd1.join(rdd2)\n",
    "print(result.collect())\n",
    "#[('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leftOuterJoin(), rightOuterJoin() # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 외부조인을 수행\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result1 = rdd1.leftOuterJoin(rdd2)\n",
    "result2 = rdd1.rightOuterJoin(rdd2)\n",
    "print(f'leftOuterJoin: {result1.collect()}')\n",
    "print(f'rightOuterJoin: {result2.collect()}')\n",
    "# leftOuterJoin: [('b', (1, 2)), ('c', (1, 2)), ('d', (1, None)), ('a', (1, None)), ('e', (1, None))]\n",
    "# rightOuterJoin: [('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtractByKey()  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\"]).map(lambda v: (v, 1))\n",
    "result = rdd1.subtractByKey(rdd2)\n",
    "print(result.collect())\n",
    "# [('a', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 집계와 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey()  # RDD구성요소 key, value에서만 가능\n",
    "# 2개의 값을 하나로 합치는 함수를 인자로 전달받는데, 이 함수는 요소들이 결합, 교환법칙 성립되어야 한다\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldByKey()   # RDD구성요소 key, value에서만 가능\n",
    "# reduceByKey와는 달리 병합연산의 초기값을 메서드의 인자로 전달해서 병합시 사용\n",
    "# ex) 더하는 함수 0, 두 문자열 연결 공백문자\"\"를 초기값으로 사용가능\n",
    "# 하지만 이 때 초기값이 반복해도 연산결과에 영향을 주지 않는 값이여야 함\n",
    "# 함수는 교환법칙은 만족안해도 되고 결합법칙은 만족해야 함\n",
    "\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.foldByKey(0, lambda v1,v2:v1+v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error  # page 137\n",
    "# combineByKey()    # RDD구성요소 key, value에서만 가능\n",
    "class Record:\n",
    "        \n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "        \n",
    "# combineBy\n",
    "def createCombiner(v):\n",
    "    return Record(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeValue(c, v):\n",
    "    return c.addAmt(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeCombiners(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "rdd = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n",
    "result = rdd.combineByKey(lambda v: createCombiner(v), lambda c, v: mergeValue(c, v), lambda c1, c2: mergeCombiners(c1, c2))\n",
    "print('Math', result.collectAsMap()['Math'], 'Eng', result.collectAsMap()['Eng'])\n",
    "#print(str(result.collectAsMap()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateByKey()   # RDD구성요소 key, value에서만 가능\n",
    "# combineByKey()와 동일하나 초기값 생성하는 부분만 다름\n",
    "# result = rdd.aggregateByKey(zero)(mergeValue, mergecombiners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipe 및 파티션과 관련된 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe()\n",
    "rdd = sc.parallelize( [\"1,2,3\", \"4,5,6\", \"7,8,9\"])\n",
    "result = rdd.pipe(\"cut -f 1,3 -d ,\") # -field  1,3출력 구분자 -d=delimeter default tab, 여기서는 ,\n",
    "print(result.collect())\n",
    "# ['1,3', '4,6', '7,9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 설정한 partition 수 조정필요시\n",
    "# coalesce()     : 줄이기만 가능         셔플 수행하라는 옵션 지정하지 않는 : 셔플을 사용하지 않기 때문\n",
    "# repartition() : 늘리거나 줄일 수 있음  셔플 기반으로 동작\n",
    "rdd1= sc.parallelize(list(range(1, 11)),10)\n",
    "rdd2 = rdd1.coalesce(5)\n",
    "rdd3 = rdd2.repartition(10)\n",
    "print(f'partition size: {rdd1.getNumPartitions()}')\n",
    "print(f'partition size: {rdd2.getNumPartitions()}')\n",
    "print(f'partition size: {rdd3.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 142\n",
    "# repartitionAndSortWithinPartitions()    # RDD구성요소 key, value에서만 가능\n",
    "# RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리, 각 파티션단위로 정렬수행한 뒤, \n",
    "# 그 결과로 새로운  RDD을  생성\n",
    "# 메서드 실행시 각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너(Partitioner)를 설정\n",
    "# 파티셔너는 각 데이터의 키 값을 이용해 데이터가 속할 파티션 결정, 키 값을 이용한 정렬도 함께 수행\n",
    "# 즉 파티션 재할당을 위해 셔플을 수행하는 단계에서 정렬도 함께 다루게 되어 파티션과 정렬 따로하는 것보다 높은 성능\n",
    "\n",
    "import random\n",
    "data = [random.randrange(1,100) for i in range(0, 10)]\n",
    "#print(data) # [23, 99, 61, 18, 84, 58, 75, 13, 55, 47]\n",
    "\n",
    "rdd1 = sc.parallelize(data).map(lambda v: (v, \"-\"))\n",
    "#print(rdd1.collect())  # [(73, '-'), (79, '-'), (47, '-'), (98, '-'), (33, '-'), (51, '-'), (43, '-'), (68, '-'), (95, '-'), (69, '-')]\n",
    "\n",
    "rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x: x) # HashPartitioner 파티셔너 3\n",
    "# print(rdd2) # PythonRDD[187] at RDD at PythonRDD.scala:53\n",
    "# rdd2.count # <bound method RDD.count of PythonRDD[315] at RDD at PythonRDD.scala:53>\n",
    "\n",
    "# 아래코드 에러는 없으나 결과 출력물이 안나옴  # cli에서는 나옴 (foreach관련) # foreach(함수)\n",
    "# rdd2.foreachPartition(lambda values: print(list(values))) # RDD의 파티션단위로 특정함수를 실행\n",
    "# [Stage 0:> (0 + 1) / 1][(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "rdd2.foreachPartition(  lambda values: print( list( map(lambda v: v, values)) ) ) \n",
    "# [(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "\n",
    "#아래코드 에러는 없으나 결과 출력물이 안나옴   # cli 에서는 나옴 (foreach)\n",
    "# def test(it):\n",
    "#     print(\"=\"*5)\n",
    "#     print ( list( map(lambda x: x, it) ) )\n",
    "# rdd2.foreachPartition(test) \n",
    "# =====\n",
    "# [(6, '-'), (42, '-'), (42, '-'), (90, '-')]\n",
    "# =====\n",
    "# [(49, '-'), (52, '-'), (58, '-')]\n",
    "# =====\n",
    "# [(35, '-'), (53, '-'), (89, '-')]\n",
    "\n",
    "\n",
    "# 아래코드는 Py4JJavaErro # cli에서도 error # foreach 2번\n",
    "# rdd2.foreachPartition( lambda it: it.foreach(lambda v: print(v))  ) # RDD의 파티션단위로 특정함수를 실행\n",
    "\n",
    "# 아래코드는 Py4JJavaErro # error\n",
    "# def test(it):\n",
    "#     print(\"=\"*5)\n",
    "#     it.foreach( lambda x: print(x) )             \n",
    "# rdd2.foreachPartition(test)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitionBy()   # RDD구성요소 key, value에서만 가능 \n",
    "# 파티셔너(Partitioner) 인자로 전달\n",
    "# HashPartitioner   : 각 요소의 키 값으로부터 해시값을 취해 이 값을 기준으로 파티션을 결정하는 방법\n",
    "# Rangepartitioner : 순서가 있는 요소들(Sortable)로 구성된 RDD에 사용가능,\n",
    "#                    각 요소를 목표 파티션 크기에 맞게 일정크기의 구간으로 나누는 방식\n",
    "\n",
    "rdd1 = sc.parallelize( [ (\"apple\", 1), (\"mouse\", 1), (\"monitor\", 1)],  5 )\n",
    "rdd2 = rdd1.partitionBy(3)\n",
    "print(f'rdd1: {rdd1.getNumPartitions()} ')  # rdd1: 5 \n",
    "print(f'rdd2: {rdd2.getNumPartitions()} ')  # rdd2: 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list ( map (함수, iter) ) 연습\n",
    "\n",
    "# li = [1, 2, 3]\n",
    "# result = map(lambda i: i * i, li)\n",
    "# print(next(result))  # 1\n",
    "# print(next(result))  # 1\n",
    "# print( list(map(lambda i: i * i, li)) )  #[1, 4, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필터와 정렬 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter()   # python filter # list( filter(함수,iter) )\n",
    "rdd1 = sc.parallelize(range(1,6))\n",
    "rdd2 = rdd1.filter(lambda i : i > 2)\n",
    "print(rdd2.collect())\n",
    "# [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortByKey()   # RDD구성요소 key, value에서만 가능\n",
    "# 키 값을 기준으로 요소를 정렬하는 연산  # 소팅 완료 후 파티션 내부의 요소는 소팅 순서상 인접한요소로 재구성\n",
    "rdd = sc.parallelize( [(\"q\",1), (\"z\", 1), (\"a\", 1)])\n",
    "result = rdd.sortByKey()\n",
    "print(result.collect())\n",
    "# [('a', 1), ('q', 1), ('z', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys()  키에 해당하는 요소로 구성된 RDD 생성\n",
    "# values() 값에 해당하는 요소로 구성된 RDD 생성\n",
    "rdd = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\" k3\", \"v3\")])\n",
    "result1 = rdd.keys()\n",
    "result2 = rdd.values()\n",
    "print(result1.collect()) # ['k1', 'k2', ' k3']\n",
    "print(result2.collect()) # ['v1', 'v2', 'v3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample()\n",
    "# 스칼라 API    sample(withReplacement: Boolean, fraction: Double, seed: Long=Utils.nextLong): RDD[t]\n",
    "# withReplacement : 복원추출 수행여부 True, False \n",
    "# fraction 복원일경우 : 샘플내에서 각 요소가 나타나는 횟수에 대한 기대값, 즉 각요소의 평균발생 횟수, 0이상\n",
    "#          비복원일경우:          각 요소가 샘플에 포함될 확률, 0 ~ 1 사이 값으로 지정가능\n",
    "\n",
    "# 샘플의 크기를 정해놓고 추출을 실행하지 않음  fraction 0.5 전체크기 50% 샘플추출아님\n",
    "# 크기를 정해놓은 메소드 takeSample()\n",
    "# seed : 일반적인 무작위 값 추출시 사용하는 것과 유사한 개념으로 반복시행시 결과가 바뀌지 않도록 제어 목적\n",
    "\n",
    "rdd = sc.parallelize(range(1, 101))\n",
    "result1 = rdd.sample(False, 0.5, 666) \n",
    "result2 = rdd.sample(True, 1.5, 777)\n",
    "print(f'result1.count(): {result1.count()}  result1.take(5): {result1.take(5)}') \n",
    "# result1.count(): 54  result1.take(5): [2, 4, 5, 6, 7]\n",
    "print(f'result2.count(): {result2.count()}  result2.take(5): {result2.take(5)}') \n",
    "# result2.count(): 153  result2.take(5): [1, 4, 4, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTION \n",
    "## 출력과 관련된 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation  # 함수형 프로그래밍 자주 활용  \n",
    "# 액션연산시 트랜스포메이션 연산도 반복수행됨. # 반복 수행 성능을 개선하기 위한 방안(캐시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first()\n",
    "# RDD요소가운데 첫번째 요소 하나를 return\n",
    "rdd = sc.parallelize( [5, 4, 1])\n",
    "result = rdd.first()\n",
    "print(result) # 5   # first() Action이므로 value return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take()\n",
    "# RDD의 첫번째 요소부터 n개를 추출해서 return\n",
    "rdd = sc.parallelize (range(1,21), 5)\n",
    "print ( rdd.take(5) ) # [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeSample()\n",
    "# RDD 요소 가운데 지정된 크기의 샘플을 추출하는 메서드\n",
    "# sample()과 차이: 1. 샘플의 크기 지정가능   2. 결과 타입이 RDD가 아닌 배열이나 리스트같은 컬렉션 타입\n",
    "\n",
    "rdd = sc.parallelize(range(1, 101))\n",
    "result = rdd.takeSample(False, 20, 666)\n",
    "print(result)\n",
    "# [76, 26, 83, 72, 93, 42, 92, 50, 18, 46, 21, 70, 40, 65, 67, 87, 7, 62, 54, 37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect(), count()\n",
    "# countByValue()  RDD에 속하는 각 값들이 나타나는 횟수를 구해서 맵(dict) 형태로 돌려주는 메서드\n",
    "rdd = sc.parallelize( [\"a\",\"a\",\"b\",\"c\",\"c\",])\n",
    "result = rdd.countByValue()\n",
    "print(result) # defaultdict(<class 'int'>, {'a': 2, 'b': 1, 'c': 2})\n",
    "for k, v in result.items():\n",
    "    print(k,v)\n",
    "# a 2\n",
    "# b 1\n",
    "# c 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce()\n",
    "# RDD에 포함된 임의의 값 2개를 하나로 합치는 함수를 이용해, RDD에 모든 요소를 하나의 값으로 병합하고 그 결과 반환\n",
    "# 각 서버에 흩어져 있는 파티션 단위로 처리 ==> RDD 모든 요소 교환,결합법칙 성립에만 사용 가능 (파티션별로 계산하므로, 나누기 안됨)\n",
    "rdd = sc.parallelize(range(1, 11), 3)\n",
    "result = rdd.reduce(lambda v1, v2: v1+v2)  # 1+2+3  4+5+6   7+8+9\n",
    "print(result) # 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold()   # reduce()와 동일하고\n",
    "# 병합 연산의 초기값 지정해 줄 수 있다 : 덧셈 0, 곱셈 1\n",
    "rdd = sc.parallelize(range(1, 11), 3)\n",
    "result = rdd.fold(0, lambda v1, v2: v1+v2)\n",
    "print(result) # 55\n",
    "\n",
    "# 파이썬 코드없어 작성 error py4jJavaError  # page 155 \n",
    "class Prod():\n",
    "    #self.cnt = 1\n",
    "    def __init__(self, price):\n",
    "        self.price = price\n",
    "        self.cnt = 1\n",
    "def reduce_(p1, p2):\n",
    "        p1.price += p2.price\n",
    "        p1.cnt += 1   \n",
    "def fold_(p1, p2) :\n",
    "    p1.price += p2.price\n",
    "    p1.cnt += 1    \n",
    "def reduceVsFold(sc):\n",
    "    rdd = sc.parallelize([Prod(300), Prod(200), Prod(100)], 10) # 파티션이 10ea, 객체 3ea\n",
    "    # print(rdd) #ParallelCollectionRDD[35] at parallelize at PythonRDD.scala:195\n",
    "    \n",
    "    r1 = rdd.reduce(reduce_)\n",
    "    print(f'Reduce: {r1.price}, {r1.cnt}')\n",
    "    \n",
    "    r2 = rdd.fold(Prod(0), fold_)  # 초기값에 의해 요소가 없더라도 초기값으로 인핸 연산수행\n",
    "    print(f'fold: {r2.price}, {r2.cnt}')    \n",
    "    \n",
    "reduceVsFold(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate()   # error Py4JJavaError\n",
    "# reduce() fold() 모두 입력과 출력 타입이 동일해야 한다는 제약, aggregate() 입출력 타입이 다를 경우 사용가능\n",
    "# 인자 3ea : 초기값, seqOp 각 파티션단위 부분합 병합함수, combOp 최종적 하나로 합치는 병합함수\n",
    "# seqOp, combOp 이전 단계의 병합 결과로 생성된 객체를 재사용하기에 매번 새로운 객체가 생성되는 부담을 덜 수 있다\n",
    "\n",
    "# error Py4JJavaError\n",
    "sc.stop()\n",
    "class Record:\n",
    "\n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "    \n",
    "# Aggregate\n",
    "def seqOp(r, v):\n",
    "    return r.addAmt(v)\n",
    "\n",
    "# Aggregate\n",
    "def combOp(r1, r2):\n",
    "    return r1 + r2\n",
    "\n",
    "class RDDOpSample():\n",
    "    def doAggregate(self, sc):\n",
    "        rdd = sc.parallelize([100, 80, 75, 90, 95])\n",
    "        result = rdd.aggregate(Record(0, 0), seqOp, combOp)\n",
    "        print(result)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#     conf = SparkConf()\n",
    "    sc = SparkContext(master=\"local[*]\", appName=\"RDDOpSample\")\n",
    "\n",
    "    obj = RDDOpSample()\n",
    "    obj.doAggregate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() : RDD 모든 요소가 double, Long 등 숫자타입일 경우에만 사용, 전체 합\n",
    "# 특화함수: RDD 키 값 에서 사용하는 reduceByKey(), combineByKey()\n",
    "rdd = sc.parallelize(range(1,11))\n",
    "result = rdd.sum()\n",
    "print(result) # 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(), foreachPartition()\n",
    "# 모든 요소에 하나씩 특정함수(인자)를 적용하는 메서드 : 실행결과 돌려주지 않음 \n",
    "# 각 word node에서 동작하므로 print()사용하면 각 work node 콘솔에 보여준다\n",
    "# mapPartition(), mapPartitionsWithIndex() : 실행 결과 돌려줌\n",
    "\n",
    "# cli에서 실행 (python3)\n",
    "def sideEffect(values):\n",
    "    print(\"Partition side Effect\")\n",
    "    for v in values:\n",
    "        print(f'Value Side Effect:{v}')\n",
    "rdd = sc.parallelize(range(1,11), 3)\n",
    "result = rdd.foreach(lambda v: print(f'Value Side Effect: {v}'))\n",
    "# [Stage 0:> (0 + 1) / 3]Value Side Effect: 1\n",
    "# Value Side Effect: 2\n",
    "# Value Side Effect: 3\n",
    "# Value Side Effect: 4\n",
    "# Value Side Effect: 5\n",
    "# Value Side Effect: 6\n",
    "# Value Side Effect: 7\n",
    "# Value Side Effect: 8\n",
    "# Value Side Effect: 9\n",
    "# Value Side Effect: 10\n",
    "\n",
    "result = rdd.foreachPartition(sideEffect)\n",
    "# Partition side Effect\n",
    "# Value Side Effect:1\n",
    "# Value Side Effect:2\n",
    "# Value Side Effect:3\n",
    "# Partition side Effect\n",
    "# Value Side Effect:4\n",
    "# Value Side Effect:5\n",
    "# Value Side Effect:6\n",
    "# Partition side Effect\n",
    "# Value Side Effect:7\n",
    "# Value Side Effect:8\n",
    "# Value Side Effect:9\n",
    "# Value Side Effect:10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toDebugString()\n",
    "# RDD 파티션 개수나 의존성 정보 등 세부정보를 알고 싶을 때\n",
    "rdd = sc.parallelize(range(1, 101), 10).map(lambda x: x*2).persist().map(lambda x:x+1).coalesce(2)\n",
    "print(rdd.toDebugString())\n",
    "# b'(2) CoalescedRDD[10] at coalesce at NativeMethodAccessorImpl.java:0 []\\n    # 최종 파티션 2\n",
    "# |  PythonRDD[9] at RDD at PythonRDD.scala:53 []\\n\n",
    "# |  PythonRDD[8] at RDD at PythonRDD.scala:53 []\\n\n",
    "# |  ParallelCollectionRDD[7] at parallelize at PythonRDD.scala:195 []'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache()    RDD의 데이터를 메모리에 저장하라는 의미로, 메모리 부족시 그 부족한 용량만큼 수행하지 않음\n",
    "# persist()  StorageLevel이라는 옵션을 이용해 저장위치와 저장방식(직렬화 여부) 등을 상세히 지정할 수 있음\n",
    "# unpersist() 이미 저장중인 데이터가 더 이상 필요없을 때 캐시설정 취소\n",
    "rdd = sc.parallelize(range(1, 101), 10)\n",
    "rdd.cache()\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "# PythonRDD[14] at RDD at PythonRDD.scala:53\n",
    "\n",
    "# 캐시정첵\n",
    "# 스파크는 메모리를 일정한 비율로 데이터 처리위한 공간, 캐시 저장 공간으로 활용 => 모두 부족시 처리위한 공간에 우선 할당\n",
    "# 데이터의 특성과 용량, 사용 가능한 클러스터 자원을 신중히 고려하여 정책 수립해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitions()  : RDD의 파티션 정보가 담긴 배열을 돌려줌  # 파이썬은 사용할 수 없음\n",
    "# getNumPartitions() : 단순한 크기정보만 알아 볼 목적에   # 파이썬 가능\n",
    "\n",
    "rdd = sc.parallelize(range(1, 101), 10)\n",
    "#print(rdd.partitions())\n",
    "print(rdd.getNumPartitions()) #10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 데이터 불러오기와 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일포맷 : 텍스트 파일, json, 하둡의 시퀸스파일, csv\n",
    "# 파일시스템 : local, HDFS, AWS의 S3, 오픈스택의 swift\n",
    "# SQL, NoSQL : MySQL,        HBase, 카산드라, Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pyspark/work\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pyspark/work\n",
      "/home/pyspark/work\n",
      "['334', '335', '336', '337', '338']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일  # textFile()  # 파티션 지정 가능\n",
    "# file://path\n",
    "# hdfs://master:port/path/...\n",
    "# s3n://bucket/path\n",
    "# 모든 서버에 지정된 경로를 통해 지정한 파일에 접근할 수 있어야 한다\n",
    "\n",
    "rdd = sc.parallelize(range(1, 1001), 3)\n",
    "codec = \"org.apache.hadoop.io.compress.GzipCodec\" # 압축(gzip)  # gzip, Snappy, bzip2, LZO 가능\n",
    "\n",
    "# save   # 동일한 폴더있으면 error # 다시 실행시 삭제후 실행  # dataframe, dataset에서는 append or overwrite기능 제공\n",
    "rdd.saveAsTextFile(\"./sub1\")         # part-00000.crc ,,3개 파일로 나뉘어 저장됨(하둡과 같은 방법)\n",
    "rdd.saveAsTextFile(\"./sub2\", codec) # 압축(gzip)\n",
    "\n",
    "# path\n",
    "# path=%pwd\n",
    "# print(path) # /home/pyspark/work\n",
    "# py용 path\n",
    "import os\n",
    "path=os.getcwd()\n",
    "print(path) # home/pyspark/work\n",
    "\n",
    "#load\n",
    "rdd2 = sc.textFile(f'file://{path}/sub1')\n",
    "print(rdd2.take(5))\n",
    "# ['334', '335', '336', '337', '338']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pyspark/work\n",
      "[334, 335, 336, 337, 338]\n"
     ]
    }
   ],
   "source": [
    "# 오브젝트 파일 objectFile  # 파이썬에서는 saveAsPickleFile(), pickleFile() 사용\n",
    "# SparkContext()의 objectFile(), RDD의 saveAsObjectFile() : object 직렬화방법을 이용해 RDD 구성하는 요소를 파일에 읽고 쓰기 기능 수행\n",
    "# 자주 사용하지 않는 이유 : 속도가 느리고 변경에 취약하다\n",
    "import os\n",
    "path=os.getcwd()\n",
    "print(path) # home/pyspark/work\n",
    "\n",
    "rdd = sc.parallelize(range(1, 1000), 3)\n",
    "rdd.saveAsPickleFile(f'file://{path}/object') # 저징\n",
    "rdd2 = sc.pickleFile(f'file://{path}/object') # 저장된 RDD의 타입으로 , 모두 string으로 읽어지는 것이 아님\n",
    "print(rdd2.take(5))\n",
    "# [334, 335, 336, 337, 338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 1\n",
      "c 1\n",
      "b 1\n",
      "c 1\n"
     ]
    }
   ],
   "source": [
    "# Sequence file\n",
    "# 키와 값으로 구성된 데이터를 저장하는 이진Binary 파일 포맷, 하둡의 대표적 파일 포맷\n",
    "# 오브젝트 파일(자바 표준 직렬화)도 이진 파일 포맷이지만, sequence는 하둡 자체 프레임워크\n",
    "# 오브젝트 파일 : 자바의 Serializable 인터페이스 구현, # sequence 파일 RDD 데이터는 하둡의 Writable 인터페이스 구현\n",
    "\n",
    "import os\n",
    "path=os.getcwd()\n",
    "path=path+'/'+'python'  # 새로운 폴더를 만들어 주어야 에러나지 않음 # 다시 실행시 삭제후 실행해야 함\n",
    "# print(path) # home/pyspark/work\n",
    "\n",
    "outputFormatClass = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
    "inputFormatClass = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
    "keyClass = \"org.apache.hadoop.io.Text\"\n",
    "valueClass = \"org.apache.hadoop.io.IntWritable\"\n",
    "conf = \"org.apache.hadoop.conf.Configuration\"\n",
    "\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"])\n",
    "rdd2 = rdd1.map(lambda x: (x, 1))\n",
    "\n",
    "# save\n",
    "rdd2.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass, valueClass)\n",
    "\n",
    "#load\n",
    "rdd3 = sc.newAPIHadoopFile(path, inputFormatClass, keyClass, valueClass)\n",
    "for k, v in rdd3.collect():\n",
    "    print(k, v)\n",
    "# a 1\n",
    "# b 1\n",
    "# c 1\n",
    "# b 1\n",
    "# c 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u1']\n"
     ]
    }
   ],
   "source": [
    "# 클러스터 환경에서의 공유 변수\n",
    "# 다수의 프로세스가 공유할 수 있는 읽기 자원과 쓰기 자원을 설정할 수 있도록 지원\n",
    "# 하둡 : 분산캐시(distributedCache)와 카운터(counter)   ===================> 단순\n",
    "# 스파크 : 브로드캐스트(Broadcast Variables)와 어큐물레이터(Accumulators) => 범용\n",
    "# Broadcast Variables : 읽기 전용 자원 (ex 온라인 쇼핑몰 : set컬렉션 타딥의 데이터 공유변수 설정, 각 서버에서 로그처리하면서 사용자확인)\n",
    "# Accumulators Variables : 쓰기 자원 (ex 온라인 쇼핑몰: 기록할 로그형식 정함)\n",
    "\n",
    "bu = sc.broadcast(set([\"u1\", \"u2\"]))\n",
    "rdd = sc.parallelize([\"u1\", \"u3\", \"u3\", \"u4\", \"u5\", 'u6'], 3)\n",
    "result = rdd.filter(lambda v: v in bu.value)\n",
    "print(result.collect())\n",
    "# ['u1']\n",
    "\n",
    "# 스파크는 액션 연산을 수행시 동일한 스테이지(stage)내에서 실행되는 태스크(task)간에는 동작에 필요한 변수를 자동으로\n",
    "# 브로드캐스트 변수를 지정하지 않아도 된다. 즉 여러 스테이지에 활용되는 경우가 아니면 굳이 명시적으로 브로드캐스트 변수 지정할 필요없음\n",
    "# 액션 연산이 수행될 때 reduceByKey()와 같이 셔플이 발생돼야 하는 경우마다 새로운 스테이지가 시작됨.\n",
    "# 즉 어떤 연산을 수행하는 과정에서 셔플을 발생시키지 않고 로컬 서버내에서 처리될 수 있는 태스크는 하나의 스테이지에 속함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어큐뮬레이터 Accumulators\n",
    "# ex) 모든 서버의 오류정보를 모아서 볼수 있게하는 것 => 각 서버에서 발생하는 특정 이벤트의 수를 세거나 관찰하고 싶은 정보\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, AccumulatorParam\n",
    "from builtins import isinstance\n",
    "\n",
    "class Record:\n",
    "\n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "    \n",
    "class AccumulatorSample():\n",
    "\n",
    "    # ex 2-142\n",
    "    def runBuitInAcc(self, sc):\n",
    "        acc1 = sc.accumulator(0)\n",
    "        data = [\"U1:Addr1\", \"U2:Addr2\", \"U3\", \"U4:Addr4\", \"U5;Addr5\", \"U6:Addr6\", \"U7::Addr7\"]\n",
    "        rdd = sc.parallelize(data)\n",
    "        rdd.foreach(lambda v: accumulate(v, acc1))\n",
    "        print(acc1.value)\n",
    "\n",
    "    # ex 2-145\n",
    "    def runCustomAcc(self, sc):\n",
    "        acc = sc.accumulator(Record(0), RecordAccumulatorParam())\n",
    "        data = [\"U1:Addr1\", \"U2:Addr2\", \"U3\", \"U4:Addr4\", \"U5;Addr5\", \"U6:Addr6\", \"U7::Addr7\"]\n",
    "        rdd = sc.parallelize(data)\n",
    "        rdd.foreach(lambda v: accumulate(v, acc))\n",
    "        print(acc.value.amount)\n",
    "\n",
    "\n",
    "def accumulate(v, acc):\n",
    "    if (len(v.split(\":\")) != 2):\n",
    "        acc.add(1)\n",
    "\n",
    "\n",
    "class RecordAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, initialValue):\n",
    "        return Record(0)\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        if (isinstance(v2, Record)):\n",
    "            return v1 + v2\n",
    "        else:\n",
    "            return v1.addAmt(v2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf()\n",
    "    sc = SparkContext(master=\"local[*]\", appName=\"AccumulatorSample\", conf=conf)\n",
    "\n",
    "    obj = AccumulatorSample()\n",
    "    obj.runBuitInAcc(sc)  # 3\n",
    "    #obj.runCustomAcc(sc)  # error\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
