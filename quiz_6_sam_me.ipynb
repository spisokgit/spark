{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력 데이터:\n",
    "* 다운로드: https://github.com/hyerica-bdml/class-2020-datascience/tree/master/Week%208/input.zip\n",
    "* 설명 - 영화DB사이트 (MovieLens)에서 수집된 데이터로서 u.data에는 사용자가 영화에 매긴 평점이 담겨있고 u.item에는 영화마다 제목, 카테고리 등의 정보가 담겨 있다.\n",
    "#### 입력파일 1 - u.data\n",
    "* 라인마다 사용자의 영화평점이 기록되어 있으며 4개의 속성 \"user id \\t item id \\t rating \\t timestamp\"가 탭으로 구분되어 작성되어 있다. (즉, split(\"\\t\") 로 자를 수 있음)\n",
    "* 모두 integer값이다.\n",
    "* 한 사람이 여러개의 영화에 평점을 매길 수 있으므로 같은 user id로 시작하는 라인이 복수번 나타날수 있다.\n",
    "* item id는 아래 u.item파일에 있는 영화 아이디 (movie id)를 가리킨다 (즉, foreign key)\n",
    "* rating은 1에서 5까지 범위이다.\n",
    "\n",
    "***힌트*** Data frame으로 읽는 방법: \n",
    " ````python\n",
    " from pyspark.sql.types import *\n",
    " udataschema = StructType([\n",
    "    StructField(\"uid\", LongType(), False),\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"rate\",LongType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False)\n",
    "    ]) \n",
    " udata = spark.read.format(\"csv\").option(\"sep\", \"\\t\").schema(udataschema).load(\"u.data\")\n",
    " ````\n",
    "\n",
    "#### 입력파일 2 - u.item\n",
    "* 라인마다 영화 정보가 기록되어 있으며 \"movie id | movie title | release date | video release date |IMDb URL | unknown | Action | Adventure | Animation |Children's | Comedy | Crime | Documentary | Drama | Fantasy |Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |Thriller | War | Western\" 의 속성 들이 파이프라인(|)으로 구분되어 작성되어 있다.\n",
    "* 그러나 아래 문제를 보면 맨 앞의 두개 속성, movie id와 movie title 만 사용한다.\n",
    "* movie id는 이 파일 안에서 한번만 나타난다 (즉, primary key)\n",
    "\n",
    "***힌트*** Data frame으로 읽는 방법:\n",
    " ````python\n",
    " uitemschema = StructType([\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"title\",StringType(), False)\n",
    " ])\n",
    " uitem = spark.read.format(\"csv\").option(\"sep\", \"|\").schema(uitemschema).load(\"u.item\")\n",
    " ````\n",
    "## 문제 1 \n",
    "* 5점을 받은 영화 중에서 알파벳 'T'로 시작하는 영화의 개수?\n",
    "* 'T'는 대문자\n",
    "* 주의: 5점을 받은 영화중 알파벳 'T'로 시작하는 영화를 중복해서 세지 않도록!\n",
    "* 제한조건: 위 코드를 이용해 u.data와 u.item을 DataFrame으로 읽어들여 select및 join을 이용하여 계산하시오.\n",
    "* 제한조건: 마지막 action으로 답이 출력되도록 코드를 작성하시오. 즉, 최종 답을 계산하기 위해 pyspark action이 아닌 python 코드를 사용하지 마시오.\n",
    "* 힌트: filter를 수행 시 filter조건을 표현할 때 어떤 column의 부분문자열을 뽑기 위해 pyspark.sql.functions.substring를 사용할 수 있음. 예를 들어 uitem이란 data frame의 title이란 column의 첫 글자는 substring(uitem[\"title\"], 0, 1)으로 지정할 수 있음.\n",
    "\n",
    "## 문제 2 \n",
    "* 사용자들에게 1점도 받고 5점도 받은 적이 있는 영화의 제목의 길이 중 가장 긴 영화제목의 길이?\n",
    "* 동일 영화가 어떤 사용자에게는 1점, 어떤 사용자에게는 5점을 받은 경우를 말함\n",
    "* 힌트: 어떤 column의 길이의 max 를 select할 때 max 함수와 length함수를 사용.\n",
    "* 예를 들어 uitem[\"title\"]의 길이의 max는 select(max(length(uitem[\"title\"]))) 로 계산 할 수 있음\n",
    "* 제한조건: 마지막 action으로 답이 출력되도록 코드를 작성하시오. 즉, 최종 답을 계산하기 위해 pyspark action이 아닌 python 코드를 사용하지 마시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------+\n",
      "|uid|mid|rate|timestamp|\n",
      "+---+---+----+---------+\n",
      "|196|242|   3|881250949|\n",
      "|186|302|   3|891717742|\n",
      "| 22|377|   1|878887116|\n",
      "+---+---+----+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- uid: long (nullable = true)\n",
      " |-- mid: long (nullable = true)\n",
      " |-- rate: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "udataschema = StructType([\n",
    "    StructField(\"uid\", LongType(), False),\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"rate\",LongType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False)\n",
    "])\n",
    "udata = spark.read.format(\"csv\").option(\"sep\", \"\\t\").schema(udataschema).load(\"u.data\")\n",
    "print(udata.show(3))\n",
    "udata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|mid|            title|\n",
      "+---+-----------------+\n",
      "|  1| Toy Story (1995)|\n",
      "|  2| GoldenEye (1995)|\n",
      "|  3|Four Rooms (1995)|\n",
      "+---+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- mid: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uitemschema = StructType([\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"title\",StringType(), False)\n",
    "])\n",
    "uitem = spark.read.format(\"csv\").option(\"sep\", \"|\").schema(uitemschema).load(\"u.item\")\n",
    "print(uitem.show(3))\n",
    "uitem.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+---+----+---------+\n",
      "|mid|               title|uid|mid|rate|timestamp|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "|242|        Kolya (1996)|196|242|   3|881250949|\n",
      "|302|L.A. Confidential...|186|302|   3|891717742|\n",
      "|377| Heavyweights (1994)| 22|377|   1|878887116|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "uitem.join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---+----+----+---------+\n",
      "| mid|               title|uid| mid|rate|timestamp|\n",
      "+----+--------------------+---+----+----+---------+\n",
      "| 465|Jungle Book, The ...|253| 465|   5|891628467|\n",
      "|1014|Romy and Michele'...|286|1014|   5|879781125|\n",
      "| 222|Star Trek: First ...|200| 222|   5|876042340|\n",
      "+----+--------------------+---+----+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem.join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\").filter( col('rate') == 5  ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+---+----+---------+\n",
      "|mid|               title|uid|mid|rate|timestamp|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "| 40|To Wong Foo, Than...|210| 40|   3|891035994|\n",
      "|118|      Twister (1996)|291|118|   2|874833878|\n",
      "|  1|    Toy Story (1995)|308|  1|   4|887736532|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem.join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\").filter( col('title')[0:1] == 'T').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+---+----+---------+\n",
      "|mid|               title|uid|mid|rate|timestamp|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "| 40|To Wong Foo, Than...|210| 40|   3|891035994|\n",
      "|118|      Twister (1996)|291|118|   2|874833878|\n",
      "|  1|    Toy Story (1995)|308|  1|   4|887736532|\n",
      "+---+--------------------+---+---+----+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem.join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\").filter( substring(uitem[\"title\"], 0, 1) == 'T').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mid=427, title='To Kill a Mockingbird (1962)', uid=60, mid=427, rate=5, timestamp=883326620),\n",
       " Row(mid=96, title='Terminator 2: Judgment Day (1991)', uid=42, mid=96, rate=5, timestamp=881107178),\n",
       " Row(mid=195, title='Terminator, The (1984)', uid=44, mid=195, rate=5, timestamp=878347874),\n",
       " Row(mid=195, title='Terminator, The (1984)', uid=72, mid=195, rate=5, timestamp=880037702),\n",
       " Row(mid=23, title='Taxi Driver (1976)', uid=59, mid=23, rate=5, timestamp=888205300)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem\\\n",
    "    .join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    "    .filter(col('rate') == '5')\\           # filter(udata[\"rate\"] == 5)\\\n",
    "    .filter( col('title')[0:1] == 'T')\\    # filter(substring(uitem[\"title\"], 0, 1) == \"T\")\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mid=427), Row(mid=96), Row(mid=195), Row(mid=195), Row(mid=23)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem\\\n",
    "    .join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    "    .filter( col('rate') == '5')\\\n",
    "    .filter( col('title')[0:1] == 'T')\\            # .filter( (col('rate') == 5) & (col('title')[0:1] == 'T') )\\\n",
    "    .select( udata['mid'] ).take(5)\n",
    "#     .select( uitem['mid'] ).take(5) # 동일한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem\\\n",
    "    .join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    "    .filter( col('rate') == '5')\\\n",
    "    .filter( col('title')[0:1] == 'T')\\\n",
    "    .select( uitem['mid'] )\\\n",
    "    .distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uitem\\\n",
    "    .join(udata, uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    "    .filter( (col('rate') == 5) & (col('title')[0:1] == 'T') )\\\n",
    "    .distinct().count() # 전체 row비교하므로 같은 것이 많이 없다 ==> select ( uitem['mid']) 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uitem.select(\"mid\", \"title\")\\\n",
    ".join(udata.select(\"mid\", \"rate\"), uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    ".filter( (col('rate') == 5) & (col('title')[0:1] == 'T') )\\\n",
    ".distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제2 : 1점도 받고 5점도 받은 영화의 제목의 길이중 가장 긴 영화제목의 길이는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(length(title))|\n",
      "+------------------+\n",
      "|                81|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem.select(\"mid\", \"title\")\\\n",
    "    .join(udata.select(\"mid\", \"rate\"), uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    "    .select(\"title\", \"rate\")\\\n",
    "    .filter( col('rate') == (1 & 5) )\\\n",
    "    .select( max( length( col( \"title\") ) )).show() # 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(length(title))|\n",
      "+------------------+\n",
      "|                81|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udata1 = udata.filter(udata[\"rate\"] == 1).select(\"mid\").distinct()\n",
    "udata2 = udata.filter(udata[\"rate\"] == 5).select(\"mid\").distinct()\n",
    "udata1\\\n",
    "    .join(udata2, \"mid\")\\\n",
    "    .join(uitem, udata1[\"mid\"] == uitem[\"mid\"])\\\n",
    "    .select(max(length(uitem[\"title\"])))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
