{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Quiz\n",
    "* TrainValidationSplit를 이용하여 영화리뷰 긍/부정 예측  \n",
    "  Estimator pipeline을 테스트(trainRatio = 0.8)\n",
    "* ParamGridBuilder를 사용하여 Word2Vec의 파라미터 vectorSize를 5,10,20,40으로 바꾸어 정확도를 측정하여 출력\n",
    "* 정확도는 BinaryClassificationEvaluator를 사용할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"my app1\").master(\"local\").getOrCreate()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"quiz_9\") \\\n",
    "                  .setMaster(\"local[*]\") \\\n",
    "                  .set(\"spark.driver.memory\", \"12g\") \\\n",
    "                  .set(\"spark.executor.memory\", \"12g\")\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from datetime import datetime   # system time \n",
    "import time                     # 수행시간\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|I grew up (b. 196...|    0|\n",
      "|When I put this m...|    0|\n",
      "|Why do people who...|    0|\n",
      "|Even though I hav...|    0|\n",
      "|Im a die hard Dad...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "40000\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.format('csv').option('header','true').option('escape','\"').load('imdb-review-sentiment.csv')\n",
    "# root\n",
    "#  |-- text: string (nullable = true)\n",
    "#  |-- label: string (nullable = true)\n",
    "\n",
    "df = spark.read.csv(\"imdb-review-sentiment.csv\", inferSchema=True, header=True, escape='\"')\n",
    "# root\n",
    "#  |-- text: string (nullable = true)\n",
    "#  |-- label: integer (nullable = true)\n",
    "print(df.show(5))\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.na.drop('any')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class RemoveStopWordsAndSpecialCharacters(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, stopwords=None):\n",
    "        super(RemoveStopWordsAndSpecialCharacters, self).__init__()\n",
    "        self.stopwords = Param(self, \"stopwords\", \"\")\n",
    "        self._setDefault(stopwords=set())\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def setStopwords(self, value):\n",
    "        self._paramMap[self.stopwords] = value\n",
    "        return self\n",
    "\n",
    "    def getStopwords(self):\n",
    "        return self.getOrDefault(self.stopwords)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        stopwords = self.getStopwords()\n",
    "\n",
    "        def f(s):\n",
    "            return [ ''.join(e for e in token if e.isalnum()) for token in s if token not in stopwords ]\n",
    "\n",
    "        t = ArrayType(StringType())\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, Word2Vec, VectorAssembler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "cleaning = RemoveStopWordsAndSpecialCharacters(inputCol=\"words\", outputCol=\"clean_words\",stopwords=stopwords)\n",
    "hashingTF = HashingTF(inputCol=\"clean_words\", outputCol=\"tf\")\n",
    "\n",
    "preprocessing = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    cleaning,\n",
    "    hashingTF,\n",
    "])\n",
    "\n",
    "preprocessed_df = preprocessing.fit(df).transform(df)\n",
    "train_set, test_set = preprocessed_df.randomSplit([0.8, 0.2], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(inputCol=\"clean_words\", outputCol=\"w2v\", minCount=1, maxIter=2)\n",
    "asm = VectorAssembler(inputCols=[hashingTF.getOutputCol(), w2v.getOutputCol()], outputCol=\"features\")\n",
    "svm = LinearSVC(labelCol=\"label\")\n",
    "\n",
    "estimator = Pipeline(stages=[\n",
    "    w2v,\n",
    "    asm,\n",
    "    svm\n",
    "])\n",
    "\n",
    "# mypipeline = Pipeline(stages=[tokenizer, cleaning, hashingTF, w2v, asm, svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "paramgrid = ParamGridBuilder().addGrid(w2v.vectorSize, [5,10, 20, 40]).build()\n",
    "# tvs = TrainValidationSplit(estimator = mypipeline, estimatorParamMaps=paramgrid, evaluator = evaluator, trainRatio=0.8, seed=46, collectSubModels=True))\n",
    "# tvsmodel=tvs.fit(df)\n",
    "# evaluator.evaluate(tvsModel.transform(df))\n",
    "tvs = TrainValidationSplit(estimator = estimator, estimatorParamMaps=paramgrid, evaluator = evaluator, trainRatio=0.8, seed=46, collectSubModels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsmodel=tvs.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i, subModel in enumerate(tvsmodel.subModels):\n",
    "    print(i)\n",
    "    test_pred = subModel.transform(test_set)\n",
    "    N = test_pred.count()\n",
    "    \n",
    "    acc.append(\n",
    "        test_pred.filter(test_pred[\"prediction\"] == test_pred[\"label\"]).count() / N\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when vector size is 5: 0.8888750311798453\n",
      "Accuracy when vector size is 10: 0.8893739087054128\n",
      "Accuracy when vector size is 20: 0.8861312047892242\n",
      "Accuracy when vector size is 40: 0.8846345722125218\n"
     ]
    }
   ],
   "source": [
    "for _acc, param in zip(acc, [5, 10, 20,40]):\n",
    "    print(f\"Accuracy when vector size is {param}: {_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수행시간(20core, memory 24G) : 197분 44.19초\n"
     ]
    }
   ],
   "source": [
    "spend_time=(time.time() - start_time)\n",
    "print(f'수행시간(20core, memory 24G) : {int(spend_time//60)}분 {spend_time%60:.2f}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 아래코드는 공부용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|null|  X2|\n",
      "|  3|   B|null|\n",
      "|  1|    |  X3|\n",
      "|  2|    |  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|null|null|\n",
      "|  1|    |    |\n",
      "|  2|    |    |\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "3\n",
      "4\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  1|    |  X3|\n",
      "|  2|    |  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|    |    |\n",
      "|  2|    |    |\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|null|  X2|\n",
      "|  3|   B|null|\n",
      "|  1|    |  X3|\n",
      "|  2|    |  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|null|null|\n",
      "|  1|    |    |\n",
      "|  2|    |    |\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  3|   B|null|\n",
      "|  3|   C|  X2|\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  3|   C|  X2|\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  1|    |  X3|\n",
      "|  2|    |  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|    |    |\n",
      "|  2|    |    |\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  3|   C|  X2|\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"A\", \"X1\"),\n",
    "    (2, None, \"X2\"),\n",
    "    (3, \"B\", None),\n",
    "    (1, \"\", \"X3\"),\n",
    "    (2, \"\", \"X2\"),\n",
    "    (3, \"C\", \"X2\"),\n",
    "    (1, None, None),\n",
    "    (1, \"\", \"\"),\n",
    "    (2, \"\", \"\"),\n",
    "    (1, \"X3\", \"X8\"),\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "print(df1.show())\n",
    "print(df1.rdd.map(lambda row: 1 if sum([c == None for c in row]) > 0 else 0  ).reduce(lambda x, y : x + y))\n",
    "print(df1.rdd.map(lambda row: 1 if sum([c == '' for c in row]) > 0 else 0  ).reduce(lambda x, y : x + y))\n",
    "print(df1.na.drop('any').show())\n",
    "print(df1.na.drop('all').show())\n",
    "print(df1.filter( df1.TYPE != '' ).show())\n",
    "print(df1.filter(  (df1.TYPE != '') &  (df1.CODE != '') ).show())\n",
    "print(df1.na.drop('any').show())\n",
    "cond = (df1.TYPE != '') & (df1.CODE != '')\n",
    "df1.filter( cond  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|null|  X2|\n",
      "|  3|   B|null|\n",
      "|  1|null|  X3|\n",
      "|  2|null|  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|null|null|\n",
      "|  1|null|null|\n",
      "|  2|null|null|\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n",
      "None\n",
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  3|   C|  X2|\n",
      "|  1|  X3|  X8|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### NULL, Empty String Blank가 포함된 ROW는 모두 제거\n",
    "\n",
    "import pyspark.sql.functions as fn\n",
    "# columns = set(df.columns) - set(['ID'])\n",
    "# cond = map(lambda x: (col(x).isNotNull()), df.columns)\n",
    "# [Column<(ID IS NOT NULL)>, Column<(TYPE IS NOT NULL)>, Column<(CODE IS NOT NULL)>]\n",
    "\n",
    "# cond = map(lambda x: (col(x)) != \"\", columns)\n",
    "# [Column<(NOT (CODE = ))>, Column<(NOT (TYPE = ))>]\n",
    "\n",
    "# cond = map(lambda x: (col(x).isNotNull()) & (col(x) != \"\"), columns)\n",
    "# cond\n",
    "# cond = reduce((lambda x, y: x & y), cond)\n",
    "# Column<(((CODE IS NOT NULL) AND (NOT (CODE = ))) AND ((TYPE IS NOT NULL) AND (NOT (TYPE = ))))>\n",
    "# df.rdd.filter(cond).take(5)\n",
    "\n",
    "for c in df1.columns:\n",
    "    df1 = df1.withColumn(c, fn.when(  fn.col(c) == ''     , None).otherwise(fn.col(c))  )\n",
    "print(df1.show())\n",
    "df1.na.drop('any').show()\n",
    "# df.filter( df.rdd.map(lambda x: (col(x).isNotNull()) & (col(x) != \"\"))    ).show()\n",
    "\n",
    "### NULL, Empty String, Blank가 포함된 ROW만 선택\n",
    "\n",
    "# cond = map(lambda x: (col(x).isNull()) | (col(x) == \"\"), df.columns)\n",
    "# cond = reduce((lambda x, y: x | y), cond)\n",
    "# df.filter(cond).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|I grew up (b. 196...|    0|\n",
      "|When I put this m...|    0|\n",
      "|Why do people who...|    0|\n",
      "|Even though I hav...|    0|\n",
      "|Im a die hard Dad...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "40000\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|I grew up (b. 196...|    0|\n",
      "|When I put this m...|    0|\n",
      "|Why do people who...|    0|\n",
      "|Even though I hav...|    0|\n",
      "|Im a die hard Dad...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "for c in df.columns:\n",
    "    df2 = df.withColumn(c, fn.when(  (fn.col(c) == '') , None).otherwise(fn.col(c))  )\n",
    "print(df2.show(5))\n",
    "print(df2.count())\n",
    "print(df2.na.drop('any').show(5))\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
