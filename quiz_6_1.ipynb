{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 1  10점  \n",
    "입력 데이터: 다운로드: https://github.com/hyerica-bdml/class-2020-datascience/tree/master/Week%208/input.zip  \n",
    "설명 - 영화DB사이트 (MovieLens)에서 수집된 데이터로서 u.data에는 사용자가 영화에 매긴 평점이 담겨있고 u.item에는 영화마다 제목, 카테고리 등의 정보가 담겨 있다.  \n",
    "입력파일 1 - u.data  \n",
    "라인마다 사용자의 영화평점이 기록되어 있으며 4개의 속성 \"user id \\t item id \\t rating \\t timestamp\"가 탭으로 구분되어 작성되어 있다. (즉, split(\"\\t\") 로 자를 수 있음)  \n",
    "모두 integer값이다.  \n",
    "한 사람이 여러개의 영화에 평점을 매길 수 있으므로 같은 user id로 시작하는 라인이 복수번 나타날수 있다.  \n",
    "item id는 아래 u.item파일에 있는 영화 아이디 (movie id)를 가리킨다 (즉, foreign key)   \n",
    "rating은 1에서 5까지 범위이다.  \n",
    "\n",
    "입력파일 2 - u.item  \n",
    "라인마다 영화 정보가 기록되어 있으며 \"movie id | movie title | release date | video release date |  \n",
    "IMDb URL | unknown | Action | Adventure | Animation |  \n",
    "Children's | Comedy | Crime | Documentary | Drama | Fantasy |  \n",
    "Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |  \n",
    "Thriller | War | Western\" 의 속성 들이 파이프라인(|)으로 구분되어 작성되어 있다.  \n",
    "그러나 아래 문제를 보면 맨 앞의 두개 속성, movie id와 movie title 만 사용한다.  \n",
    "movie id는 이 파일 안에서 한번만 나타난다 (즉, primary key)  \n",
    "\n",
    "문제.\n",
    "5점을 받은 영화 중에서 알파벳 'T'로 시작하는 영화의 개수?\n",
    "\n",
    "'T'는 대문자  \n",
    "주의: 5점을 받은 영화중 알파벳 'T'로 시작하는 영화를 중복해서 세지 않도록!  \n",
    "제한조건: 위 코드를 이용해 u.data와 u.item을 DataFrame으로 읽어들여 select및 join을 이용하여 계산하시오.  \n",
    "제한조건: 마지막 action으로 답이 출력되도록 코드를 작성하시오. 즉, 최종 답을 계산하기 위해 pyspark action이 아닌 python 코드를 사용하지 마시오.  \n",
    "힌트: filter를 수행 시 filter조건을 표현할 때 어떤 column의 부분문자열을 뽑기 위해 pyspark.sql.functions.substring를 사용할 수 있음.   \n",
    "    예를 들어 uitem이란 data frame의 title이란 column의 첫 글자는 substring(uitem\\[\"title\"], 0, 1)으로 지정할 수 있음.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate() \n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---------+\n",
      "|uid|mid|rate|timestamp|\n",
      "+---+---+----+---------+\n",
      "|196|242|   3|881250949|\n",
      "|186|302|   3|891717742|\n",
      "| 22|377|   1|878887116|\n",
      "+---+---+----+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- uid: long (nullable = true)\n",
      " |-- mid: long (nullable = true)\n",
      " |-- rate: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"uid\", LongType(), False),\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"rate\", LongType(), False),\n",
    "    StructField(\"timestamp\", LongType(), False)\n",
    "])\n",
    "udata = spark.read.format(\"csv\").option(\"sep\", \"\\t\" ).schema(schema).load(\"u.data\")\n",
    "print(udata.show(3))\n",
    "udata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|mid|            title|\n",
      "+---+-----------------+\n",
      "|  1| Toy Story (1995)|\n",
      "|  2| GoldenEye (1995)|\n",
      "|  3|Four Rooms (1995)|\n",
      "+---+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- mid: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"mid\", LongType(), False),\n",
    "    StructField(\"title\", StringType(), False)\n",
    "])\n",
    "uitem = spark.read.format(\"csv\").option(\"sep\", \"|\").schema(schema).load(\"u.item\")\n",
    "print(uitem.show(3))\n",
    "uitem.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "uitem.select(\"mid\", \"title\")\\\n",
    ".join(udata.select(\"mid\", \"rate\"), uitem[\"mid\"] == udata[\"mid\"], \"inner\")\\\n",
    ".filter( (col('rate') == 5) & (col('title')[0:1] == 'T') )\\\n",
    ".distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
