{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=RDDOpSample, master=local[*]) created by __init__ at <ipython-input-5-19b2ba8b5fd4>:371 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-19b2ba8b5fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.host\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"127.0.0.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RDDOpSample\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDDOpSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=RDDOpSample, master=local[*]) created by __init__ at <ipython-input-5-19b2ba8b5fd4>:371 "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "class Record:\n",
    "\n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "# MapPartions\n",
    "def increase(numbers):\n",
    "    print(\"DB 연결 !!!\")\n",
    "    return (i + 1 for i in numbers)\n",
    "\n",
    "\n",
    "# MapPartitionsWithIndex\n",
    "def increaseWithIndex(idx, numbers):\n",
    "    for i in numbers:\n",
    "        if (idx == 1):\n",
    "            yield i + 1\n",
    "\n",
    "\n",
    "# combineBy\n",
    "def createCombiner(v):\n",
    "    return Record(v)\n",
    "\n",
    "\n",
    "# combineBy\n",
    "def mergeValue(c, v):\n",
    "    return c.addAmt(v)\n",
    "\n",
    "\n",
    "# combineBy\n",
    "def mergeCombiners(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "\n",
    "# Aggregate\n",
    "def seqOp(r, v):\n",
    "    return r.addAmt(v)\n",
    "\n",
    "\n",
    "# Aggregate\n",
    "def combOp(r1, r2):\n",
    "    return r1 + r2\n",
    "\n",
    "\n",
    "# foreachPartition\n",
    "def sideEffect(values):\n",
    "    print(\"Partition Side Effect!!\")\n",
    "    for v in values:\n",
    "        print(\"Value Side Effect: %s\" % v)\n",
    "\n",
    "\n",
    "class RDDOpSample():\n",
    "\n",
    "    def doCollect(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11))\n",
    "        result = rdd.collect()\n",
    "        print(result)\n",
    "\n",
    "    def doCount(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11))\n",
    "        result = rdd.count()\n",
    "        print(result)\n",
    "\n",
    "    def doMap(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 6))\n",
    "        rdd2 = rdd1.map(lambda v: v + 1)\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doFlatMap(self, sc):\n",
    "        rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "        rdd2 = rdd1.flatMap(lambda s: s.split(\",\"))\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doMapPartitions(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 11), 3)\n",
    "        rdd2 = rdd1.mapPartitions(increase)\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doMapPartitionsWithIndex(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 11), 3)\n",
    "        rdd2 = rdd1.mapPartitionsWithIndex(increaseWithIndex)\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doMapValues(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "        rdd2 = rdd1.map(lambda v: (v, 1))\n",
    "        rdd3 = rdd2.mapValues(lambda i: i + 1)\n",
    "        print(rdd3.collect())\n",
    "\n",
    "    def doFlatMapValues(self, sc):\n",
    "        rdd1 = sc.parallelize([(1, \"a,b\"), (2, \"a,c\"), (1, \"d,e\")])\n",
    "        rdd2 = rdd1.flatMapValues(lambda s: s.split(\",\"))\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doZip(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "        rdd2 = sc.parallelize([1, 2, 3])\n",
    "        result = rdd1.zip(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doGroupBy(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 11))\n",
    "        rdd2 = rdd1.groupBy(lambda v: \"even\" if v % 2 == 0 else \"odd\")\n",
    "        for x in rdd2.collect():\n",
    "            print(x[0], list(x[1]))\n",
    "\n",
    "    def doGroupByKey(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"]).map(lambda v: (v, 1))\n",
    "        rdd2 = rdd1.groupByKey()\n",
    "        for x in rdd2.collect():\n",
    "            print(x[0], list(x[1]))\n",
    "\n",
    "    def doCogroup(self, sc):\n",
    "        rdd1 = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k1\", \"v3\")])\n",
    "        rdd2 = sc.parallelize([(\"k1\", \"v4\")])\n",
    "        result = rdd1.cogroup(rdd2)\n",
    "        for x in result.collect():\n",
    "            print(x[0], list(x[1][0]), list(x[1][1]))\n",
    "\n",
    "    def doDistinct(self, sc):\n",
    "        rdd = sc.parallelize([1, 2, 3, 1, 2, 3, 1, 2, 3])\n",
    "        result = rdd.distinct()\n",
    "        print(result.collect())\n",
    "\n",
    "    def doCartesian(self, sc):\n",
    "        rdd1 = sc.parallelize([1, 2, 3])\n",
    "        rdd2 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "        result = rdd1.cartesian(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doSubtract(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "        rdd2 = sc.parallelize([\"d\", \"e\"])\n",
    "        result = rdd1.subtract(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doUnion(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "        rdd2 = sc.parallelize([\"d\", \"e\", \"f\"])\n",
    "        result = rdd1.union(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doIntersection(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"a\", \"b\", \"c\"])\n",
    "        rdd2 = sc.parallelize([\"a\", \"a\", \"c\", \"c\"])\n",
    "        result = rdd1.intersection(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doJoin(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v, 1))\n",
    "        rdd2 = sc.parallelize([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "        result = rdd1.join(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doLeftOuterJoin(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\"]).map(lambda v: (v, 1))\n",
    "        rdd2 = sc.parallelize([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "        result1 = rdd1.leftOuterJoin(rdd2)\n",
    "        result2 = rdd1.rightOuterJoin(rdd2)\n",
    "        print(\"Left: %s\" % result1.collect())\n",
    "        print(\"Right: %s\" % result2.collect())\n",
    "\n",
    "    def doSubtractByKey(self, sc):\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\"]).map(lambda v: (v, 1))\n",
    "        rdd2 = sc.parallelize([\"b\"]).map(lambda v: (v, 1))\n",
    "        result = rdd1.subtractByKey(rdd2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doReduceByKey(self, sc):\n",
    "        rdd = sc.parallelize([\"a\", \"b\", \"b\"]).map(lambda v: (v, 1))\n",
    "        result = rdd.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doFoldByKey(self, sc):\n",
    "        rdd = sc.parallelize([\"a\", \"b\", \"b\"]).map(lambda v: (v, 1))\n",
    "        result = rdd.foldByKey(0, lambda v1, v2: v1 + v2)\n",
    "        print(result.collect())\n",
    "\n",
    "    def doCombineByKey(self, sc):\n",
    "        rdd = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n",
    "        result = rdd.combineByKey(lambda v: createCombiner(v), lambda c, v: mergeValue(c, v),\n",
    "                                  lambda c1, c2: mergeCombiners(c1, c2))\n",
    "        print('Math', result.collectAsMap()['Math'], 'Eng', result.collectAsMap()['Eng'])\n",
    "\n",
    "    def doAggregateByKey(self, sc):\n",
    "        rdd = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n",
    "        result = rdd.aggregateByKey(Record(0, 0), lambda c, v: mergeValue(c, v), lambda c1, c2: mergeCombiners(c1, c2))\n",
    "        print('Math', result.collectAsMap()['Math'], 'Eng', result.collectAsMap()['Eng'])\n",
    "\n",
    "    def doPipe(self, sc):\n",
    "        rdd = sc.parallelize([\"1,2,3\", \"4,5,6\", \"7,8,9\"])\n",
    "        result = rdd.pipe(\"cut -f 1,3 -d ,\")\n",
    "        print(result.collect())\n",
    "\n",
    "    def doCoalesceAndRepartition(self, sc):\n",
    "        rdd1 = sc.parallelize(list(range(1, 11)), 10)\n",
    "        rdd2 = rdd1.coalesce(5)\n",
    "        rdd3 = rdd2.repartition(10)\n",
    "        print(\"partition size: %d\" % rdd1.getNumPartitions())\n",
    "        print(\"partition size: %d\" % rdd2.getNumPartitions())\n",
    "        print(\"partition size: %d\" % rdd3.getNumPartitions())\n",
    "\n",
    "    def doRepartitionAndSortWithinPartitions(self, sc):\n",
    "        data = [random.randrange(1, 100) for i in range(0, 10)]\n",
    "        rdd1 = sc.parallelize(data).map(lambda v: (v, \"-\"))\n",
    "        rdd2 = rdd1.repartitionAndSortWithinPartitions(3, lambda x: x)\n",
    "        rdd2.foreachPartition(lambda values: print(list(values)))\n",
    "\n",
    "    def doPartitionBy(self, sc):\n",
    "        rdd1 = sc.parallelize([(\"apple\", 1), (\"mouse\", 1), (\"monitor\", 1)], 5)\n",
    "        rdd2 = rdd1.partitionBy(3)\n",
    "        print(\"rdd1: %d, rdd2: %d\" % (rdd1.getNumPartitions(), rdd2.getNumPartitions()))\n",
    "\n",
    "    def doFilter(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 6))\n",
    "        rdd2 = rdd1.filter(lambda i: i > 2)\n",
    "        print(rdd2.collect())\n",
    "\n",
    "    def doSortByKey(self, sc):\n",
    "        rdd = sc.parallelize([(\"q\", 1), (\"z\", 1), (\"a\", 1)])\n",
    "        result = rdd.sortByKey()\n",
    "        print(result.collect())\n",
    "\n",
    "    def doKeysAndValues(self, sc):\n",
    "        rdd = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k3\", \"v3\")])\n",
    "        print(rdd.keys().collect())\n",
    "        print(rdd.values().collect())\n",
    "\n",
    "    def doSample(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 101))\n",
    "        result1 = rdd.sample(False, 0.5, 100)\n",
    "        result2 = rdd.sample(True, 1.5, 100)\n",
    "        print(result1.take(5))\n",
    "        print(result2.take(5))\n",
    "\n",
    "    def doFirst(self, sc):\n",
    "        rdd = sc.parallelize([5, 4, 1])\n",
    "        result = rdd.first()\n",
    "        print(result)\n",
    "\n",
    "    def doTake(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 100))\n",
    "        result = rdd.take(5)\n",
    "        print(result)\n",
    "\n",
    "    def doTakeSample(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 100))\n",
    "        result = rdd.takeSample(False, 20)\n",
    "        print(len(result))\n",
    "\n",
    "    def doCountByValue(self, sc):\n",
    "        rdd = sc.parallelize([1, 1, 2, 3, 3])\n",
    "        result = rdd.countByValue()\n",
    "        for k, v in result.items():\n",
    "            print(k, \"->\", v)\n",
    "\n",
    "    def doReduce(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11), 3)\n",
    "        result = rdd.reduce(lambda v1, v2: v1 + v2)\n",
    "        print(result)\n",
    "\n",
    "    def doFold(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11), 3)\n",
    "        result = rdd.fold(0, lambda v1, v2: v1 + v2)\n",
    "        print(result)\n",
    "\n",
    "    def doAggregate(self, sc):\n",
    "        rdd = sc.parallelize([100, 80, 75, 90, 95])\n",
    "        result = rdd.aggregate(Record(0, 0), seqOp, combOp)\n",
    "        print(result)\n",
    "\n",
    "    def doSum(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11))\n",
    "        result = rdd.sum()\n",
    "        print(result)\n",
    "\n",
    "    def doForeach(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11))\n",
    "        result = rdd.foreach(lambda v: print(\"Value Side Effect: %s\" % v))\n",
    "\n",
    "    def doForeachPartition(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 11), 3)\n",
    "        result = rdd.foreachPartition(sideEffect)\n",
    "\n",
    "    def doDebugString(self, sc):\n",
    "        rdd1 = sc.parallelize(range(1, 100), 10)\n",
    "        rdd2 = rdd1.map(lambda v: v * 2)\n",
    "        rdd3 = rdd2.map(lambda v: v + 1)\n",
    "        rdd4 = rdd3.coalesce(2)\n",
    "        print(rdd4.toDebugString())\n",
    "\n",
    "    def doCache(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 100), 10)\n",
    "        rdd.cache()\n",
    "        rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "        print(rdd.persist().is_cached)\n",
    "\n",
    "    def doGetPartitions(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 100), 10)\n",
    "        print(rdd.getNumPartitions())\n",
    "\n",
    "    def saveAndLoadTextFile(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 1000), 3)\n",
    "        codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "        # save\n",
    "        rdd.saveAsTextFile(\"<path_to_save>/sub1\")\n",
    "        # save(gzip)\n",
    "        rdd.saveAsTextFile(\"<path_to_save>/sub2\", codec)\n",
    "        # load\n",
    "        rdd2 = sc.textFile(\"<path_to_save>/sub1\")\n",
    "        print(rdd2.take(10))\n",
    "\n",
    "    def saveAndLoadObjectFile(self, sc):\n",
    "        rdd = sc.parallelize(range(1, 1000), 3)\n",
    "        # save\n",
    "        # 아래 경로는 실제 저장 경로로 변경하여 테스트\n",
    "        rdd.saveAsPickleFile(\"data/sample/saveAsObjectFile/python\")\n",
    "        # load\n",
    "        # 아래 경로는 실제 저장 경로로 변경하여 테스트\n",
    "        rdd2 = sc.pickleFile(\"data/sample/saveAsObjectFile/python\")\n",
    "        print(rdd2.take(10))\n",
    "\n",
    "    def saveAndLoadSequenceFile(self, sc):\n",
    "        # 아래 경로는 실제 저장 경로로 변경하여 테스트\n",
    "        path = \"data/sample/saveAsSeqFile/python\"\n",
    "\n",
    "        outputFormatClass = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
    "        inputFormatClass = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
    "        keyClass = \"org.apache.hadoop.io.Text\"\n",
    "        valueClass = \"org.apache.hadoop.io.IntWritable\"\n",
    "        conf = \"org.apache.hadoop.conf.Configuration\"\n",
    "        rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"])\n",
    "        rdd2 = rdd1.map(lambda x: (x, 1))\n",
    "        # save\n",
    "        rdd2.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass, valueClass)\n",
    "        # load\n",
    "        rdd3 = sc.newAPIHadoopFile(path, inputFormatClass, keyClass, valueClass)\n",
    "        for k, v in rdd3.collect():\n",
    "            print(k, v)\n",
    "\n",
    "    def testBroadcaset(self, sc):\n",
    "        bu = sc.broadcast(set([\"u1\", \"u2\"]))\n",
    "        rdd = sc.parallelize([\"u1\", \"u3\", \"u3\", \"u4\", \"u5\", \"u6\"], 3)\n",
    "        result = rdd.filter(lambda v: v in bu.value)\n",
    "        print(result.collect())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    sc = SparkContext(master=\"local[*]\", appName=\"RDDOpSample\", conf=conf)\n",
    "\n",
    "    obj = RDDOpSample()\n",
    "\n",
    "    # [예제 실행 방법] 아래에서 원하는 예제의 주석을 제거하고 실행!!\n",
    "    # ex) obj.testBroadcaset(sc)\n",
    "    # obj.doCollect(sc)\n",
    "    # obj.doCount(sc)\n",
    "    # obj.doMap(sc)\n",
    "    # obj.doFlatMap(sc)\n",
    "    # obj.doMapPartitions(sc)\n",
    "    # obj.doMapPartitionsWithIndex(sc)\n",
    "    # obj.doMapValues(sc)\n",
    "    # obj.doFlatMapValues(sc)\n",
    "    # obj.doZip(sc)\n",
    "    # obj.doGroupBy(sc)\n",
    "    # obj.doGroupByKey(sc)\n",
    "    # obj.doCogroup(sc)\n",
    "    # obj.doDistinct(sc)\n",
    "    # obj.doCartesian(sc)\n",
    "    # obj.doSubtract(sc)\n",
    "    # obj.doUnion(sc)\n",
    "    # obj.doIntersection(sc)\n",
    "    # obj.doJoin(sc)\n",
    "    # obj.doLeftOuterJoin(sc)\n",
    "    # obj.doSubtractByKey(sc)\n",
    "    # obj.doReduceByKey(sc)\n",
    "    # obj.doFoldByKey(sc)\n",
    "    # obj.doCombineByKey(sc)\n",
    "    # obj.doAggregateByKey(sc)\n",
    "    # obj.doPipe(sc)\n",
    "    # obj.doCoalesceAndRepartition(sc)\n",
    "    # obj.doRepartitionAndSortWithinPartitions(sc)\n",
    "    # obj.doPartitionBy(sc)\n",
    "    # obj.doFilter(sc)\n",
    "    # obj.doSortByKey(sc)\n",
    "    # obj.doKeysAndValues(sc)\n",
    "    # obj.doSample(sc)\n",
    "    # obj.doFirst(sc)\n",
    "    # obj.doTake(sc)\n",
    "    # obj.doTakeSample(sc)\n",
    "    # obj.doCountByValue(sc)\n",
    "    # obj.doReduce(sc)\n",
    "    # obj.doFold(sc)\n",
    "    obj.doAggregate(sc)\n",
    "    # obj.doSum(sc)\n",
    "    # obj.doForeach(sc)\n",
    "    # obj.doForeachPartition(sc)\n",
    "    # obj.doDebugString(sc)\n",
    "    # obj.doCache(sc)\n",
    "    # obj.doGetPartitions(sc)\n",
    "    # obj.saveAndLoadTextFile(sc)\n",
    "    # obj.saveAndLoadObjectFile(sc)\n",
    "    # obj.saveAndLoadSequenceFile(sc)\n",
    "    # obj.testBroadcaset(sc)\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
