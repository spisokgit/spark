{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "# get context from the session\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & schema configuration\n",
    "\n",
    "Field types\n",
    "* EXT1 ~ OPN10: float\n",
    "* country: string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10', 'EXT1_E', 'EXT2_E', 'EXT3_E', 'EXT4_E', 'EXT5_E', 'EXT6_E', 'EXT7_E', 'EXT8_E', 'EXT9_E', 'EXT10_E', 'EST1_E', 'EST2_E', 'EST3_E', 'EST4_E', 'EST5_E', 'EST6_E', 'EST7_E', 'EST8_E', 'EST9_E', 'EST10_E', 'AGR1_E', 'AGR2_E', 'AGR3_E', 'AGR4_E', 'AGR5_E', 'AGR6_E', 'AGR7_E', 'AGR8_E', 'AGR9_E', 'AGR10_E', 'CSN1_E', 'CSN2_E', 'CSN3_E', 'CSN4_E', 'CSN5_E', 'CSN6_E', 'CSN7_E', 'CSN8_E', 'CSN9_E', 'CSN10_E', 'OPN1_E', 'OPN2_E', 'OPN3_E', 'OPN4_E', 'OPN5_E', 'OPN6_E', 'OPN7_E', 'OPN8_E', 'OPN9_E', 'OPN10_E', 'dateload', 'screenw', 'screenh', 'introelapse', 'testelapse', 'endelapse', 'IPC', 'country', 'lat_appx_lots_of_err', 'long_appx_lots_of_err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([ StructField(col, StringType(), False) if col == 'country' else StructField(col, FloatType(), False) for col in cols ]  )\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\", \"\\t\").schema(schema).load(\"data-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = cols[:50] + ['country']\n",
    "cols_to_drop = [ col for col in cols if col not in cols_to_use ]\n",
    "raw_data = raw_data.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT1: float (nullable = true)\n",
      " |-- EXT2: float (nullable = true)\n",
      " |-- EXT3: float (nullable = true)\n",
      " |-- EXT4: float (nullable = true)\n",
      " |-- EXT5: float (nullable = true)\n",
      " |-- EXT6: float (nullable = true)\n",
      " |-- EXT7: float (nullable = true)\n",
      " |-- EXT8: float (nullable = true)\n",
      " |-- EXT9: float (nullable = true)\n",
      " |-- EXT10: float (nullable = true)\n",
      " |-- EST1: float (nullable = true)\n",
      " |-- EST2: float (nullable = true)\n",
      " |-- EST3: float (nullable = true)\n",
      " |-- EST4: float (nullable = true)\n",
      " |-- EST5: float (nullable = true)\n",
      " |-- EST6: float (nullable = true)\n",
      " |-- EST7: float (nullable = true)\n",
      " |-- EST8: float (nullable = true)\n",
      " |-- EST9: float (nullable = true)\n",
      " |-- EST10: float (nullable = true)\n",
      " |-- AGR1: float (nullable = true)\n",
      " |-- AGR2: float (nullable = true)\n",
      " |-- AGR3: float (nullable = true)\n",
      " |-- AGR4: float (nullable = true)\n",
      " |-- AGR5: float (nullable = true)\n",
      " |-- AGR6: float (nullable = true)\n",
      " |-- AGR7: float (nullable = true)\n",
      " |-- AGR8: float (nullable = true)\n",
      " |-- AGR9: float (nullable = true)\n",
      " |-- AGR10: float (nullable = true)\n",
      " |-- CSN1: float (nullable = true)\n",
      " |-- CSN2: float (nullable = true)\n",
      " |-- CSN3: float (nullable = true)\n",
      " |-- CSN4: float (nullable = true)\n",
      " |-- CSN5: float (nullable = true)\n",
      " |-- CSN6: float (nullable = true)\n",
      " |-- CSN7: float (nullable = true)\n",
      " |-- CSN8: float (nullable = true)\n",
      " |-- CSN9: float (nullable = true)\n",
      " |-- CSN10: float (nullable = true)\n",
      " |-- OPN1: float (nullable = true)\n",
      " |-- OPN2: float (nullable = true)\n",
      " |-- OPN3: float (nullable = true)\n",
      " |-- OPN4: float (nullable = true)\n",
      " |-- OPN5: float (nullable = true)\n",
      " |-- OPN6: float (nullable = true)\n",
      " |-- OPN7: float (nullable = true)\n",
      " |-- OPN8: float (nullable = true)\n",
      " |-- OPN9: float (nullable = true)\n",
      " |-- OPN10: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "| 4.0| 1.0| 5.0| 2.0| 5.0| 1.0| 5.0| 2.0| 4.0|  1.0| 1.0| 4.0| 4.0| 2.0| 2.0| 2.0| 2.0| 2.0| 3.0|  2.0| 2.0| 5.0| 2.0| 4.0| 2.0| 3.0| 2.0| 4.0| 3.0|  4.0| 3.0| 4.0| 3.0| 2.0| 2.0| 4.0| 4.0| 2.0| 4.0|  4.0| 5.0| 1.0| 4.0| 1.0| 4.0| 1.0| 5.0| 3.0| 4.0|  5.0|     GB|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.filter((raw_data['country'] == 'KR') | (raw_data['country'] == 'US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0| 5.0| 3.0| 4.0|  3.0| 2.0| 4.0| 4.0| 2.0| 4.0| 2.0| 2.0| 2.0| 4.0|  4.0| 1.0| 2.0| 1.0| 5.0| 3.0| 5.0| 3.0| 4.0| 4.0|  5.0| 3.0| 2.0| 4.0| 2.0| 1.0| 4.0| 4.0| 2.0| 2.0|  5.0| 5.0| 2.0| 4.0| 3.0| 4.0| 1.0| 5.0| 5.0| 4.0|  4.0|     US|\n",
      "| 3.0| 2.0| 2.0| 4.0| 4.0| 4.0| 5.0| 3.0| 1.0|  3.0| 3.0| 3.0| 4.0| 4.0| 3.0| 3.0| 5.0| 4.0| 3.0|  4.0| 1.0| 5.0| 1.0| 4.0| 5.0| 3.0| 2.0| 3.0| 4.0|  2.0| 3.0| 3.0| 2.0| 2.0| 2.0| 2.0| 4.0| 3.0| 2.0|  2.0| 3.0| 4.0| 3.0| 2.0| 2.0| 5.0| 3.0| 2.0| 1.0|  2.0|     US|\n",
      "| 1.0| 2.0| 3.0| 4.0| 3.0| 3.0| 2.0| 5.0| 1.0|  5.0| 3.0| 2.0| 4.0| 1.0| 2.0| 3.0| 3.0| 3.0| 4.0|  3.0| 3.0| 3.0| 4.0| 3.0| 4.0| 5.0| 3.0| 3.0| 3.0|  2.0| 3.0| 3.0| 4.0| 2.0| 3.0| 1.0| 4.0| 1.0| 1.0|  3.0| 4.0| 4.0| 5.0| 2.0| 3.0| 1.0| 3.0| 4.0| 4.0|  4.0|     US|\n",
      "| 3.0| 4.0| 1.0| 4.0| 3.0| 2.0| 2.0| 4.0| 3.0|  5.0| 5.0| 3.0| 4.0| 3.0| 5.0| 1.0| 3.0| 2.0| 5.0|  2.0| 4.0| 2.0| 5.0| 1.0| 5.0| 2.0| 4.0| 1.0| 1.0|  4.0| 2.0| 5.0| 4.0| 3.0| 1.0| 4.0| 5.0| 3.0| 3.0|  5.0| 3.0| 4.0| 2.0| 2.0| 4.0| 4.0| 5.0| 3.0| 4.0|  2.0|     US|\n",
      "| 3.0| 2.0| 5.0| 3.0| 4.0| 2.0| 4.0| 3.0| 3.0|  3.0| 4.0| 3.0| 4.0| 4.0| 2.0| 3.0| 2.0| 2.0| 2.0|  2.0| 1.0| 5.0| 1.0| 5.0| 1.0| 4.0| 1.0| 4.0| 4.0|  4.0| 5.0| 3.0| 5.0| 2.0| 4.0| 2.0| 4.0| 2.0| 4.0|  4.0| 4.0| 2.0| 4.0| 2.0| 4.0| 2.0| 4.0| 3.0| 4.0|  4.0|     US|\n",
      "| 3.0| 1.0| 5.0| 4.0| 5.0| 2.0| 3.0| 4.0| 4.0|  2.0| 1.0| 3.0| 2.0| 3.0| 2.0| 2.0| 1.0| 2.0| 2.0|  4.0| 1.0| 4.0| 2.0| 5.0| 1.0| 5.0| 1.0| 5.0| 4.0|  4.0| 2.0| 4.0| 4.0| 1.0| 3.0| 3.0| 3.0| 2.0| 4.0|  3.0| 5.0| 1.0| 5.0| 1.0| 4.0| 1.0| 5.0| 4.0| 5.0|  4.0|     US|\n",
      "| 4.0| 3.0| 1.0| 3.0| 3.0| 1.0| 4.0| 5.0| 3.0|  5.0| 5.0| 1.0| 5.0| 1.0| 1.0| 5.0| 5.0| 5.0| 5.0|  4.0| 5.0| 2.0| 4.0| 3.0| 4.0| 2.0| 3.0| 3.0| 3.0|  4.0| 1.0| 3.0| 4.0| 5.0| 3.0| 5.0| 4.0| 3.0| 2.0|  3.0| 3.0| 2.0| 5.0| 1.0| 4.0| 1.0| 3.0| 5.0| 4.0|  5.0|     US|\n",
      "| 2.0| 4.0| 3.0| 4.0| 4.0| 3.0| 2.0| 3.0| 2.0|  3.0| 5.0| 1.0| 5.0| 1.0| 5.0| 5.0| 4.0| 5.0| 5.0|  3.0| 3.0| 2.0| 4.0| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0|  3.0| 2.0| 4.0| 4.0| 2.0| 1.0| 0.0| 3.0| 4.0| 4.0|  4.0| 5.0| 4.0| 5.0| 3.0| 4.0| 1.0| 5.0| 4.0| 4.0|  4.0|     US|\n",
      "| 3.0| 1.0| 4.0| 2.0| 4.0| 2.0| 3.0| 3.0| 4.0|  3.0| 3.0| 3.0| 4.0| 2.0| 2.0| 1.0| 4.0| 3.0| 2.0|  1.0| 1.0| 5.0| 1.0| 5.0| 1.0| 5.0| 2.0| 5.0| 4.0|  4.0| 4.0| 3.0| 4.0| 2.0| 3.0| 3.0| 5.0| 1.0| 4.0|  5.0| 3.0| 2.0| 4.0| 1.0| 5.0| 1.0| 5.0| 5.0| 4.0|  5.0|     US|\n",
      "| 4.0| 2.0| 4.0| 2.0| 5.0| 2.0| 4.0| 2.0| 5.0|  2.0| 3.0| 3.0| 4.0| 1.0| 2.0| 2.0| 2.0| 2.0| 2.0|  4.0| 1.0| 5.0| 1.0| 4.0| 2.0| 4.0| 2.0| 5.0| 4.0|  4.0| 4.0| 3.0| 3.0| 2.0| 2.0| 2.0| 4.0| 2.0| 4.0|  4.0| 5.0| 2.0| 3.0| 1.0| 5.0| 2.0| 5.0| 5.0| 3.0|  4.0|     US|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing steps\n",
    "\n",
    "1. Imputer: replace <code>None</code> with average values\n",
    "1. VectorAssembler: create a feature vector with all scores for the 50 questions\n",
    "1. PCA: reduce the dimensionality into 10\n",
    "1. GaussianMixture (or KMeans): clustering\n",
    "1. StringIndexer: represent country attributes with integer IDs\n",
    "1. OneHotEncoder: interpret country IDs with one hot vectors\n",
    "1. VectorAssembler: concatenate cluster probability vector and one-hot encoded country ID vector\n",
    "1. Correlation: compute correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(inputCols=raw_data.columns[:50],outputCols=raw_data.columns[:50])\n",
    "raw_data = imputer.fit(raw_data).transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = raw_data.columns[:50]\n",
    "assembler = VectorAssembler(inputCols=input_cols,outputCol=\"raw_features\")\n",
    "df1=assembler.transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=10, inputCol=\"raw_features\", outputCol=\"features\")\n",
    "df2 = pca.fit(df1).transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gm = GaussianMixture(featuresCol=\"features\", k=2, tol=0.0001, seed=10)\n",
    "df3 = gm.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_int\")\n",
    "df4 = indexer.fit(df3).transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"country_int\", outputCol=\"country_onehot\", dropLast=False)\n",
    "df5 = encoder.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"probability\", \"country_onehot\"],outputCol=\"corr_features\")\n",
    "df6=assembler.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|corr_features                                     |\n",
      "+--------------------------------------------------+\n",
      "|[0.0024932054464815914,0.9975067945535183,1.0,0.0]|\n",
      "|[0.3921775721861428,0.6078224278138572,1.0,0.0]   |\n",
      "|[0.03323653703287065,0.9667634629671294,1.0,0.0]  |\n",
      "|[0.9357516669480505,0.06424833305194967,1.0,0.0]  |\n",
      "|[2.0509958008700714E-4,0.9997949004199129,1.0,0.0]|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(\"corr_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pearson(corr_features)=DenseMatrix(4, 4, [1.0, -1.0, -0.0112, 0.0112, -1.0, 1.0, 0.0112, -0.0112, -0.0112, 0.0112, 1.0, -1.0, 0.0112, -0.0112, -1.0, 1.0], False))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df6, 'corr_features', 'pearson').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
