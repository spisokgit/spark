{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local\", appName=\"first app\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139931149930184"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action - collect()\n",
    "rdd = sc.parallelize(range(1,11))\n",
    "result = rdd.collect()\n",
    "print(result) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "id(result)    #139931150042440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10914784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action - count()\n",
    "rdd = sc.parallelize(range(1, 11))\n",
    "result = rdd.count()\n",
    "print(result)  # 10\n",
    "id(result)     # 10914784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# transformation Action - map, collect()\n",
    "rdd1 = sc.parallelize(range(1, 6))\n",
    "rdd2= rdd1.map(lambda v : v+1)\n",
    "print(rdd2.collect())\n",
    "# [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'orange'], ['grape', 'apple', 'mango'], ['blueberry', 'tomato', 'orange']]\n"
     ]
    }
   ],
   "source": [
    "# map \n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.map(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [['apple', 'orange'], ['grape', 'apple', 'mango'], ['blueberry', 'tomato', 'orange']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'orange', 'grape', 'apple', 'mango', 'blueberry', 'tomato', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple,mango\", \"blueberry,tomato,orange\"])\n",
    "rdd2 = rdd1.flatMap(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# ['apple', 'orange', 'grape', 'apple', 'mango', 'blueberry', 'tomato', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap return type TraversableOnce[U]\n",
    "rdd1 = sc.parallelize([\"apple,orange\", \"grape,apple.mango\", \"blueberry,tomato,orange\"])\n",
    "def deflog(log):\n",
    "    if \"apple\" in log:\n",
    "        return list[log]\n",
    "    else:\n",
    "        return list()\n",
    "rdd2 = rdd1.flatMap(deflog)  # transformation에서는 error발생치 않음\n",
    "print(rdd2)                  # PythonRDD[12] at RDD at PythonRDD.scala:53\n",
    "#print(rdd2.collect())       #  deflog 'type' object is not subscriptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions\n",
    "# map()과 flatMap() RDD의 각 요소를 하나씩 처리\n",
    "# mapPartition() 파티션단위로 처리:파티션에 속한 모든 요소의 컬렉션에 대한 이터레이터(Iterator)를 입력으로 사용,리턴도 이터레이터\n",
    "# 파티션 단위의 중간산출물을 만들거나 DB 연결과 같은 고비용의 자원을 파티션 단위로 공유해 사용할 수 있다는 장점\n",
    "\n",
    "# increase\n",
    "def increase(numbers):\n",
    "    print(\"DB 연결 !!!\")\n",
    "    return(i + 1 for i in numbers)\n",
    "\n",
    "#rdd1 = sc.parallelize(range(1,11))   # 옵션으로 partition 정할 수 있다\n",
    "rdd1 = sc.parallelize(range(1,11), 3)   # 옵션으로 partition 정할 수 있다\n",
    "rdd2 = rdd1.mapPartitions(increase)    # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "#[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "# 파티션에 속한 요소의 정보 + 해당파티션의 인덱스 함께 전달\n",
    "#increaseWithIndex\n",
    "def increaseWithIndex(idx, numbers):\n",
    "    print(\"partitions !!!\")\n",
    "    for i in numbers:\n",
    "        if(idx == 1):\n",
    "            yield i + 1  # yield 일시정지 변수기억 next()\n",
    "\n",
    "            \n",
    "rdd1 = sc.parallelize(range(1,11), 3)\n",
    "rdd2 = rdd1.mapPartitionsWithIndex(increaseWithIndex)  # 함수자체를 매개변수로\n",
    "print(rdd2.collect())\n",
    "# [5, 6, 7]  <== [ 4, 5, 6] 파티션이 index 1로 전달되었음을 추정할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 2), ('c', 2)]\n"
     ]
    }
   ],
   "source": [
    "# mapValues  # RDD의 요소가 key value의 쌍으로 이루는 경우 페어RDD(PairRDD)\n",
    "# 인자로 받은 value에 해당하는 요소에만 적용, 그 결과로 구성된 새로운 RDD 생성\n",
    "\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.map(lambda v : (v, 1))\n",
    "\n",
    "rdd3 = rdd2.mapValues(lambda i: i + 1)\n",
    "print(rdd3.collect())\n",
    "# [('a', 2), ('b', 2), ('c', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'c'), (1, 'd'), (1, 'e')]\n"
     ]
    }
   ],
   "source": [
    "# flatMapValues\n",
    "\n",
    "rdd1 = sc.parallelize([(1, \"a,b\"), (2, \"a,c\"), (1, \"d,e\")])\n",
    "\n",
    "# PairRDD 생성\n",
    "rdd2 = rdd1.flatMapValues(lambda s: s.split(\",\"))\n",
    "print(rdd2.collect())\n",
    "# [(1, 'a'), (1, 'b'), (2, 'a'), (2, 'c'), (1, 'd'), (1, 'e')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그룹과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.api.java.JavaPairRDD@708ff7c7\n",
      "[('a', 1), ('b', 2), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "# zip  # key value # 요소의 개수 같아야 한다\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "result = rdd1.zip(rdd2)\n",
    "print(result) #org.apache.spark.api.java.JavaPairRDD@37a7333b\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipPartitions  파티션단위로 zip()연산 수행 # 파티션 개수 같아야 한다\n",
    "# 파이썬에서는 사용할 수 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[62] at RDD at PythonRDD.scala:53\n",
      "[('odd', <pyspark.resultiterable.ResultIterable object at 0x7f4442786160>), ('even', <pyspark.resultiterable.ResultIterable object at 0x7f44427865f8>)]\n",
      "odd [1, 3, 5, 7, 9]\n",
      "even [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# groupBy # value값을 새로운 key값으로 하는 (key, value그룹(시퀸스))으로 생성\n",
    "# 인자로 전달하는 함수가 각 그룹의 키를 결정하는 역할 담당\n",
    "\n",
    "rdd1 = sc.parallelize(range(1, 11))\n",
    "rdd2 = rdd1.groupBy(lambda v: \"even\" if v%2==0 else \"odd\")\n",
    "print(rdd2) # PythonRDD[38] at RDD at PythonRDD.scala:53\n",
    "print(rdd2.collect())\n",
    "#[('odd', <pyspark.resultiterable.ResultIterable object at 0x7f444277e860>), ('even', <pyspark.resultiterable.ResultIterable object at 0x7f444277e828>)]\n",
    "for x in rdd2.collect():\n",
    "    #print(x[0], x[1])\n",
    "# odd <pyspark.resultiterable.ResultIterable object at 0x7f44432759e8>\n",
    "# even <pyspark.resultiterable.ResultIterable object at 0x7f44604d7160>\n",
    "    print(x[0], list(x[1]))\n",
    "# odd [1, 3, 5, 7, 9]\n",
    "# even [2, 4, 6, 8, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1]\n",
      "b [1, 1]\n",
      "c [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey  # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 같은 키를 가진 요소들로 그룹을 만들고\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"b\", \"c\"]).map(lambda v: (v, 1))\n",
    "rdd2 = rdd1.groupByKey()\n",
    "for x in rdd2.collect():\n",
    "    print(x[0], list(x[1]))\n",
    "# a [1]\n",
    "# b [1, 1]\n",
    "# c [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[130] at RDD at PythonRDD.scala:53\n",
      "[('k2', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7e48>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be7c50>)), ('k1', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7dd8>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be4f28>))]\n",
      "k2 ['v2'] []\n",
      "k1 ['v1', 'v3'] ['v4']\n"
     ]
    }
   ],
   "source": [
    "# cogroup   ## RDD구성요소 key, value에서만 가능\n",
    "# join과 비교\n",
    "# 여러 RDD에서 같은 키를 갖는 값 요소를 찾아서 키와 그 키에 속하는 요소의 시퀀스(List, Vector 등의 상위클래스인 Iterable)\n",
    "# 구성된 튜플, 그 튜플로 구성된 RDD 생성 \n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize([(\"k1\", \"v1\"), (\"k2\", \"v2\"), (\"k1\", \"v3\")])\n",
    "rdd2 = sc.parallelize([(\"k1\", \"v4\")])\n",
    "result = rdd1.cogroup(rdd2)\n",
    "print(result) # PythonRDD[120] at RDD at PythonRDD.scala:53\n",
    "print(result.collect())\n",
    "# [('k2', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7e48>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be7c50>)), ('k1', (<pyspark.resultiterable.ResultIterable object at 0x7f4440be7dd8>, <pyspark.resultiterable.ResultIterable object at 0x7f4440be4f28>))]\n",
    "for x in result.collect():\n",
    "    print(x[0], list(x[1][0]), list(x[1][1]))\n",
    "# k2 ['v2'] []\n",
    "# k1 ['v1', 'v3'] ['v4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집합과 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# distinct  # RDD의 원소에서 중복을 제외한 요소로만 구성된 새로운 RDD 생성\n",
    "rdd = sc.parallelize([1,2,3,1,2,3,1,2,3])\n",
    "result = rdd.distinct()\n",
    "print(result.collect())\n",
    "#[1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "result = rdd1.cartesian(rdd2)\n",
    "print(result.collect())\n",
    "#[(1, 'a'), (1, 'b'), (1, 'c'), (2, 'a'), (2, 'b'), (2, 'c'), (3, 'a'), (3, 'b'), (3, 'c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'a']\n"
     ]
    }
   ],
   "source": [
    "# subtract  # 두 개의 RDD가 있을 때 rdd1- rdd2\n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\"])\n",
    "result = rdd1.subtract(rdd2)\n",
    "print(result.collect())\n",
    "#['b', 'c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "# union \n",
    "rdd1 = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd2 = sc.parallelize([\"d\", \"e\", \"f\"])\n",
    "result = rdd1.union(rdd2)\n",
    "print(result.collect())\n",
    "#['a', 'b', 'c', 'd', 'e', 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'a']\n"
     ]
    }
   ],
   "source": [
    "# intersection  #  중복제외됨\n",
    "rdd1 = sc.parallelize ([\"a\", \"a\", \"b\", \"c\" ])\n",
    "rdd2 = sc.parallelize ([\"a\", \"a\", \"c\", \"c\"])\n",
    "result = rdd1.intersection(rdd2)\n",
    "print(result.collect())\n",
    "# ['c', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (1, 2)), ('c', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# join  # RDD구성요소 key, value에서만 가능\n",
    "# cogroup와 비교  # Action값을 바로 볼 수 있다\n",
    "# [ Tuple(key, Tuple(rdd1요소들의 집합, rdd2요소들의 집합)), ... ]\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result = rdd1.join(rdd2)\n",
    "print(result.collect())\n",
    "#[('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leftOuterJoin: [('b', (1, 2)), ('c', (1, 2)), ('d', (1, None)), ('a', (1, None)), ('e', (1, None))]\n",
      "rightOuterJoin: [('b', (1, 2)), ('c', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# leftOuterJoin, rightOuterJoin  # RDD구성요소 key, value에서만 가능\n",
    "# key기준으로 외부조인을 수행\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\", \"c\", \"d\", \"e\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\", \"c\"]).map(lambda v: (v, 2))\n",
    "result1 = rdd1.leftOuterJoin(rdd2)\n",
    "result2 = rdd1.rightOuterJoin(rdd2)\n",
    "print(f'leftOuterJoin: {result1.collect()}')\n",
    "print(f'rightOuterJoin: {result2.collect()}')\n",
    "# leftOuterJoin: [('b', (1, 2)), ('c', (1, 2)), ('d', (1, None)), ('a', (1, None)), ('e', (1, None))]\n",
    "# rightOuterJoin: [('b', (1, 2)), ('c', (1, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1)]\n"
     ]
    }
   ],
   "source": [
    "# subtractByKey  # RDD구성요소 key, value에서만 가능\n",
    "rdd1 = sc.parallelize ([\"a\", \"b\"]).map(lambda v: (v,1))\n",
    "rdd2 = sc.parallelize ([\"b\"]).map(lambda v: (v, 1))\n",
    "result = rdd1.subtractByKey(rdd2)\n",
    "print(result.collect())\n",
    "# [('a', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 집계와 관련된 연산들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey  # RDD구성요소 key, value에서만 가능\n",
    "# 2개의 값을 하나로 합치는 함수를 인자로 전달받는데, 이 함수는 결합, 교환법칙 성립\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# foldByKey   # RDD구성요소 key, value에서만 가능\n",
    "# reduceByKey와는 달리 병합연산의 초기값을 메서드의 인자로 전달해서 병합시 사용\n",
    "# ex) 더하는 함수 0, 두 문자열 연결 공백문자\"\"를 초기값으로 사용가능\n",
    "# 하지만 이 때 초기값이 반복해도 연산결과에 영향을 주지 않는 값이여야 함\n",
    "# 함수는 교환법칙은 만족안해도 되고 결합법칙은 만족해야 함\n",
    "\n",
    "rdd = sc.parallelize ([\"a\", \"b\", \"b\"]).map(lambda v: (v,1))\n",
    "result = rdd.foldByKey(0, lambda v1,v2:v1+v2)\n",
    "print(result.collect())\n",
    "# [('a', 1), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 74.0 failed 1 times, most recent failure: Lost task 0.0 in stage 74.0 (TID 92, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-517ea8fca3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Math\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Math\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Eng\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreateCombiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmergeValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmergeCombiners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Math'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Math'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Eng'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Eng'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m#print(str(result.collectAsMap()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \"\"\"\n\u001b[0;32m-> 1587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 74.0 failed 1 times, most recent failure: Lost task 0.0 in stage 74.0 (TID 92, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\", line 1796, in add_shuffle_key\n    yield outputSerializer.dumps(items)\n  File \"/home/pyspark/pkg/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 583, in dumps\n    return pickle.dumps(obj, protocol)\n_pickle.PicklingError: Can't pickle <class '__main__.Record'>: attribute lookup Record on __main__ failed\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1073)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1089)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1126)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# combineByKey    # RDD구성요소 key, value에서만 가능\n",
    "class Record:\n",
    "        \n",
    "    def __init__(self, amount, number=1):\n",
    "        self.amount = amount\n",
    "        self.number = number\n",
    "        \n",
    "    def addAmt(self, amount):\n",
    "        return Record(self.amount + amount, self.number + 1)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        amount = self.amount + other.amount\n",
    "        number = self.number + other.number \n",
    "        return Record(amount, number)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"avg:\" + str(self.amount / self.number)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Record(%r, %r)' % (self.amount, self.number)\n",
    "        \n",
    "# combineBy\n",
    "def createCombiner(v):\n",
    "    return Record(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeValue(c, v):\n",
    "    return c.addAmt(v)\n",
    "\n",
    "# combineBy\n",
    "def mergeCombiners(c1, c2):\n",
    "    return c1 + c2\n",
    "\n",
    "rdd = sc.parallelize([(\"Math\", 100), (\"Eng\", 80), (\"Math\", 50), (\"Eng\", 70), (\"Eng\", 90)])\n",
    "result = rdd.combineByKey(lambda v: createCombiner(v), lambda c, v: mergeValue(c, v), lambda c1, c2: mergeCombiners(c1, c2))\n",
    "print('Math', result.collectAsMap()['Math'], 'Eng', result.collectAsMap()['Eng'])\n",
    "#print(str(result.collectAsMap()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateByKey   # RDD구성요소 key, value에서만 가능\n",
    "# combineByKey()와 동일하나 초기값 생성하는 부분만 다름\n",
    "result = rdd.aggregateByKey(zero)(mergeValue, mergecombiners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipe 및 파티션과 관련된 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pyspark/work\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
