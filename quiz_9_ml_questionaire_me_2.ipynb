{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제\n",
    "* Input data\n",
    "    + Big five personality test records whose country is KR or US\n",
    "    + Download data-final.csv file from https://www.kaggle.com/tunguz/big-five-personality-test\n",
    "* Assignments\n",
    "    + Do clustering with KMeans and GaussianMixture\n",
    "    + For each clustering method, compute the Pearson correlation matrix between country attribute   \n",
    "    and the probability distributions (or cluster assignment) that a person belongs to Group 1 or 2\n",
    "* Code submission\n",
    "각 클러스터에 속할 확률과 응답자 국가 간의 Pearson correlation을 출력하는 jupyter notebook을 작성하고  \n",
    "파일이름을 '학생이름.ipynb'으로 하여 업로드 하시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                     # 수행시간\n",
    "start_time = time.time()\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# create spark session\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"my app\").master(\"local\").getOrCreate()\n",
    "\n",
    "conf = SparkConf().setAppName(\"quiz_9\") \\\n",
    "                  .setMaster(\"local[*]\") \\\n",
    "                  .set(\"spark.driver.memory\", \"12g\") \\\n",
    "                  .set(\"spark.executor.memory\", \"12g\")\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & schema configuration\n",
    "\n",
    "Field types\n",
    "* EXT1 ~ OPN10: float\n",
    "* country: string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['EXT1', 'EXT2', 'EXT3', 'EXT4', 'EXT5', 'EXT6', 'EXT7', 'EXT8', 'EXT9', 'EXT10', 'EST1', 'EST2', 'EST3', 'EST4', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10', 'AGR1', 'AGR2', 'AGR3', 'AGR4', 'AGR5', 'AGR6', 'AGR7', 'AGR8', 'AGR9', 'AGR10', 'CSN1', 'CSN2', 'CSN3', 'CSN4', 'CSN5', 'CSN6', 'CSN7', 'CSN8', 'CSN9', 'CSN10', 'OPN1', 'OPN2', 'OPN3', 'OPN4', 'OPN5', 'OPN6', 'OPN7', 'OPN8', 'OPN9', 'OPN10', 'EXT1_E', 'EXT2_E', 'EXT3_E', 'EXT4_E', 'EXT5_E', 'EXT6_E', 'EXT7_E', 'EXT8_E', 'EXT9_E', 'EXT10_E', 'EST1_E', 'EST2_E', 'EST3_E', 'EST4_E', 'EST5_E', 'EST6_E', 'EST7_E', 'EST8_E', 'EST9_E', 'EST10_E', 'AGR1_E', 'AGR2_E', 'AGR3_E', 'AGR4_E', 'AGR5_E', 'AGR6_E', 'AGR7_E', 'AGR8_E', 'AGR9_E', 'AGR10_E', 'CSN1_E', 'CSN2_E', 'CSN3_E', 'CSN4_E', 'CSN5_E', 'CSN6_E', 'CSN7_E', 'CSN8_E', 'CSN9_E', 'CSN10_E', 'OPN1_E', 'OPN2_E', 'OPN3_E', 'OPN4_E', 'OPN5_E', 'OPN6_E', 'OPN7_E', 'OPN8_E', 'OPN9_E', 'OPN10_E', 'dateload', 'screenw', 'screenh', 'introelapse', 'testelapse', 'endelapse', 'IPC', 'country', 'lat_appx_lots_of_err', 'long_appx_lots_of_err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([ StructField(col, StringType(), False) if col == 'country' else StructField(col, FloatType(), False) for col in cols ]  )\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\", \"\\t\").schema(schema).load(\"data-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = cols[:50] + ['country']\n",
    "cols_to_drop = [ col for col in cols if col not in cols_to_use ]\n",
    "raw_data= raw_data.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "| 4.0| 1.0| 5.0| 2.0| 5.0| 1.0| 5.0| 2.0| 4.0|  1.0| 1.0| 4.0| 4.0| 2.0| 2.0| 2.0| 2.0| 2.0| 3.0|  2.0| 2.0| 5.0| 2.0| 4.0| 2.0| 3.0| 2.0| 4.0| 3.0|  4.0| 3.0| 4.0| 3.0| 2.0| 2.0| 4.0| 4.0| 2.0| 4.0|  4.0| 5.0| 1.0| 4.0| 1.0| 4.0| 1.0| 5.0| 3.0| 4.0|  5.0|     GB|\n",
      "| 3.0| 5.0| 3.0| 4.0| 3.0| 3.0| 2.0| 5.0| 1.0|  5.0| 2.0| 3.0| 4.0| 1.0| 3.0| 1.0| 2.0| 1.0| 3.0|  1.0| 1.0| 4.0| 1.0| 5.0| 1.0| 5.0| 3.0| 4.0| 5.0|  3.0| 3.0| 2.0| 5.0| 3.0| 3.0| 1.0| 3.0| 3.0| 5.0|  3.0| 1.0| 2.0| 4.0| 2.0| 3.0| 1.0| 4.0| 2.0| 5.0|  3.0|     MY|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT1: float (nullable = true)\n",
      " |-- EXT2: float (nullable = true)\n",
      " |-- EXT3: float (nullable = true)\n",
      " |-- EXT4: float (nullable = true)\n",
      " |-- EXT5: float (nullable = true)\n",
      " |-- EXT6: float (nullable = true)\n",
      " |-- EXT7: float (nullable = true)\n",
      " |-- EXT8: float (nullable = true)\n",
      " |-- EXT9: float (nullable = true)\n",
      " |-- EXT10: float (nullable = true)\n",
      " |-- EST1: float (nullable = true)\n",
      " |-- EST2: float (nullable = true)\n",
      " |-- EST3: float (nullable = true)\n",
      " |-- EST4: float (nullable = true)\n",
      " |-- EST5: float (nullable = true)\n",
      " |-- EST6: float (nullable = true)\n",
      " |-- EST7: float (nullable = true)\n",
      " |-- EST8: float (nullable = true)\n",
      " |-- EST9: float (nullable = true)\n",
      " |-- EST10: float (nullable = true)\n",
      " |-- AGR1: float (nullable = true)\n",
      " |-- AGR2: float (nullable = true)\n",
      " |-- AGR3: float (nullable = true)\n",
      " |-- AGR4: float (nullable = true)\n",
      " |-- AGR5: float (nullable = true)\n",
      " |-- AGR6: float (nullable = true)\n",
      " |-- AGR7: float (nullable = true)\n",
      " |-- AGR8: float (nullable = true)\n",
      " |-- AGR9: float (nullable = true)\n",
      " |-- AGR10: float (nullable = true)\n",
      " |-- CSN1: float (nullable = true)\n",
      " |-- CSN2: float (nullable = true)\n",
      " |-- CSN3: float (nullable = true)\n",
      " |-- CSN4: float (nullable = true)\n",
      " |-- CSN5: float (nullable = true)\n",
      " |-- CSN6: float (nullable = true)\n",
      " |-- CSN7: float (nullable = true)\n",
      " |-- CSN8: float (nullable = true)\n",
      " |-- CSN9: float (nullable = true)\n",
      " |-- CSN10: float (nullable = true)\n",
      " |-- OPN1: float (nullable = true)\n",
      " |-- OPN2: float (nullable = true)\n",
      " |-- OPN3: float (nullable = true)\n",
      " |-- OPN4: float (nullable = true)\n",
      " |-- OPN5: float (nullable = true)\n",
      " |-- OPN6: float (nullable = true)\n",
      " |-- OPN7: float (nullable = true)\n",
      " |-- OPN8: float (nullable = true)\n",
      " |-- OPN9: float (nullable = true)\n",
      " |-- OPN10: float (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.filter((raw_data['country'] == 'KR') | (raw_data['country'] == 'US'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547996\n",
      "547990\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0| 5.0| 3.0| 4.0|  3.0| 2.0| 4.0| 4.0| 2.0| 4.0| 2.0| 2.0| 2.0| 4.0|  4.0| 1.0| 2.0| 1.0| 5.0| 3.0| 5.0| 3.0| 4.0| 4.0|  5.0| 3.0| 2.0| 4.0| 2.0| 1.0| 4.0| 4.0| 2.0| 2.0|  5.0| 5.0| 2.0| 4.0| 3.0| 4.0| 1.0| 5.0| 5.0| 4.0|  4.0|     US|\n",
      "| 3.0| 2.0| 2.0| 4.0| 4.0| 4.0| 5.0| 3.0| 1.0|  3.0| 3.0| 3.0| 4.0| 4.0| 3.0| 3.0| 5.0| 4.0| 3.0|  4.0| 1.0| 5.0| 1.0| 4.0| 5.0| 3.0| 2.0| 3.0| 4.0|  2.0| 3.0| 3.0| 2.0| 2.0| 2.0| 2.0| 4.0| 3.0| 2.0|  2.0| 3.0| 4.0| 3.0| 2.0| 2.0| 5.0| 3.0| 2.0| 1.0|  2.0|     US|\n",
      "| 1.0| 2.0| 3.0| 4.0| 3.0| 3.0| 2.0| 5.0| 1.0|  5.0| 3.0| 2.0| 4.0| 1.0| 2.0| 3.0| 3.0| 3.0| 4.0|  3.0| 3.0| 3.0| 4.0| 3.0| 4.0| 5.0| 3.0| 3.0| 3.0|  2.0| 3.0| 3.0| 4.0| 2.0| 3.0| 1.0| 4.0| 1.0| 1.0|  3.0| 4.0| 4.0| 5.0| 2.0| 3.0| 1.0| 3.0| 4.0| 4.0|  4.0|     US|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.count())\n",
    "# raw_data = raw_data.drop('country')\n",
    "raw_data = raw_data.limit(547990)\n",
    "print(raw_data.count())\n",
    "raw_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing steps\n",
    "\n",
    "1. Imputer: replace <code>None</code> with average values\n",
    "1. VectorAssembler: create a feature vector with all scores for the 50 questions\n",
    "1. PCA: reduce the dimensionality into 10\n",
    "1. GaussianMixture (or KMeans): clustering\n",
    "1. StringIndexer: represent country attributes with integer IDs\n",
    "1. OneHotEncoder: interpret country IDs with one hot vectors\n",
    "1. VectorAssembler: concatenate cluster probability vector and one-hot encoded country ID vector\n",
    "1. Correlation: compute correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(inputCols=raw_data.columns[:50],outputCols=raw_data.columns[:50])\n",
    "raw_data = imputer.fit(raw_data).transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "547990\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0| 5.0| 3.0| 4.0|  3.0| 2.0| 4.0| 4.0| 2.0| 4.0| 2.0| 2.0| 2.0| 4.0|  4.0| 1.0| 2.0| 1.0| 5.0| 3.0| 5.0| 3.0| 4.0| 4.0|  5.0| 3.0| 2.0| 4.0| 2.0| 1.0| 4.0| 4.0| 2.0| 2.0|  5.0| 5.0| 2.0| 4.0| 3.0| 4.0| 1.0| 5.0| 5.0| 4.0|  4.0|     US|\n",
      "| 3.0| 2.0| 2.0| 4.0| 4.0| 4.0| 5.0| 3.0| 1.0|  3.0| 3.0| 3.0| 4.0| 4.0| 3.0| 3.0| 5.0| 4.0| 3.0|  4.0| 1.0| 5.0| 1.0| 4.0| 5.0| 3.0| 2.0| 3.0| 4.0|  2.0| 3.0| 3.0| 2.0| 2.0| 2.0| 2.0| 4.0| 3.0| 2.0|  2.0| 3.0| 4.0| 3.0| 2.0| 2.0| 5.0| 3.0| 2.0| 1.0|  2.0|     US|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(imputer.getStrategy())\n",
    "print(raw_data.count())\n",
    "raw_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = raw_data.columns[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_cols,outputCol=\"raw_features\")\n",
    "df1=assembler.transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        raw_features|\n",
      "+--------------------+\n",
      "|[4.0,3.0,4.0,3.0,...|\n",
      "|[3.0,2.0,2.0,4.0,...|\n",
      "|[1.0,2.0,3.0,4.0,...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"raw_features\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|        raw_features|            features|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0| 5.0| 3.0| 4.0|  3.0| 2.0| 4.0| 4.0| 2.0| 4.0| 2.0| 2.0| 2.0| 4.0|  4.0| 1.0| 2.0| 1.0| 5.0| 3.0| 5.0| 3.0| 4.0| 4.0|  5.0| 3.0| 2.0| 4.0| 2.0| 1.0| 4.0| 4.0| 2.0| 2.0|  5.0| 5.0| 2.0| 4.0| 3.0| 4.0| 1.0| 5.0| 5.0| 4.0|  4.0|     US|[4.0,3.0,4.0,3.0,...|[-0.0404338023452...|\n",
      "| 3.0| 2.0| 2.0| 4.0| 4.0| 4.0| 5.0| 3.0| 1.0|  3.0| 3.0| 3.0| 4.0| 4.0| 3.0| 3.0| 5.0| 4.0| 3.0|  4.0| 1.0| 5.0| 1.0| 4.0| 5.0| 3.0| 2.0| 3.0| 4.0|  2.0| 3.0| 3.0| 2.0| 2.0| 2.0| 2.0| 4.0| 3.0| 2.0|  2.0| 3.0| 4.0| 3.0| 2.0| 2.0| 5.0| 3.0| 2.0| 1.0|  2.0|     US|[3.0,2.0,2.0,4.0,...|[3.22162795473387...|\n",
      "| 1.0| 2.0| 3.0| 4.0| 3.0| 3.0| 2.0| 5.0| 1.0|  5.0| 3.0| 2.0| 4.0| 1.0| 2.0| 3.0| 3.0| 3.0| 4.0|  3.0| 3.0| 3.0| 4.0| 3.0| 4.0| 5.0| 3.0| 3.0| 3.0|  2.0| 3.0| 3.0| 4.0| 2.0| 3.0| 1.0| 4.0| 1.0| 1.0|  3.0| 4.0| 4.0| 5.0| 2.0| 3.0| 1.0| 3.0| 4.0| 4.0|  4.0|     US|[1.0,2.0,3.0,4.0,...|[4.69501703285374...|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=10, inputCol=\"raw_features\", outputCol=\"features\")\n",
    "df2 = pca.fit(df1).transform(df1)\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "|EXT1|EXT2|EXT3|EXT4|EXT5|EXT6|EXT7|EXT8|EXT9|EXT10|EST1|EST2|EST3|EST4|EST5|EST6|EST7|EST8|EST9|EST10|AGR1|AGR2|AGR3|AGR4|AGR5|AGR6|AGR7|AGR8|AGR9|AGR10|CSN1|CSN2|CSN3|CSN4|CSN5|CSN6|CSN7|CSN8|CSN9|CSN10|OPN1|OPN2|OPN3|OPN4|OPN5|OPN6|OPN7|OPN8|OPN9|OPN10|country|        raw_features|            features|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "| 4.0| 3.0| 4.0| 3.0| 3.0| 3.0| 5.0| 3.0| 4.0|  3.0| 2.0| 4.0| 4.0| 2.0| 4.0| 2.0| 2.0| 2.0| 4.0|  4.0| 1.0| 2.0| 1.0| 5.0| 3.0| 5.0| 3.0| 4.0| 4.0|  5.0| 3.0| 2.0| 4.0| 2.0| 1.0| 4.0| 4.0| 2.0| 2.0|  5.0| 5.0| 2.0| 4.0| 3.0| 4.0| 1.0| 5.0| 5.0| 4.0|  4.0|     US|[4.0,3.0,4.0,3.0,...|[-0.0404338023452...|\n",
      "| 3.0| 2.0| 2.0| 4.0| 4.0| 4.0| 5.0| 3.0| 1.0|  3.0| 3.0| 3.0| 4.0| 4.0| 3.0| 3.0| 5.0| 4.0| 3.0|  4.0| 1.0| 5.0| 1.0| 4.0| 5.0| 3.0| 2.0| 3.0| 4.0|  2.0| 3.0| 3.0| 2.0| 2.0| 2.0| 2.0| 4.0| 3.0| 2.0|  2.0| 3.0| 4.0| 3.0| 2.0| 2.0| 5.0| 3.0| 2.0| 1.0|  2.0|     US|[3.0,2.0,2.0,4.0,...|[3.22162795473387...|\n",
      "| 1.0| 2.0| 3.0| 4.0| 3.0| 3.0| 2.0| 5.0| 1.0|  5.0| 3.0| 2.0| 4.0| 1.0| 2.0| 3.0| 3.0| 3.0| 4.0|  3.0| 3.0| 3.0| 4.0| 3.0| 4.0| 5.0| 3.0| 3.0| 3.0|  2.0| 3.0| 3.0| 4.0| 2.0| 3.0| 1.0| 4.0| 1.0| 1.0|  3.0| 4.0| 4.0| 5.0| 2.0| 3.0| 1.0| 3.0| 4.0| 4.0|  4.0|     US|[1.0,2.0,3.0,4.0,...|[4.69501703285374...|\n",
      "+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+-------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gm = GaussianMixture(featuresCol=\"features\", k=2, tol=0.0001, seed=10) # tol 조기종료 조건\n",
    "df3 = gm.fit(df2).transform(df2) # prediction,  probability 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------+\n",
      "|prediction|probability                              |\n",
      "+----------+-----------------------------------------+\n",
      "|0         |[0.9962795654227283,0.003720434577271679]|\n",
      "|1         |[0.4722966849601867,0.5277033150398134]  |\n",
      "|0         |[0.9430267462702172,0.05697325372978272] |\n",
      "+----------+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 클러스터에 속할 확률\n",
    "df3.select(\"prediction\", \"probability\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer : 한 컬럼에 주어진 모든 워드 리스트에 대해 인덱스 벡터 형성 (us, kr) => 0, 1\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_int\")\n",
    "df4 = indexer.fit(df3).transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|country|country_int|\n",
      "+-------+-----------+\n",
      "|     US|        0.0|\n",
      "|     US|        0.0|\n",
      "|     US|        0.0|\n",
      "+-------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select(\"country\", \"country_int\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"country_int\", outputCol=\"country_onehot\", dropLast=False)\n",
    "df5 = encoder.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+\n",
      "|country|country_int|country_onehot|\n",
      "+-------+-----------+--------------+\n",
      "|     US|        0.0| (2,[0],[1.0])|\n",
      "|     US|        0.0| (2,[0],[1.0])|\n",
      "|     US|        0.0| (2,[0],[1.0])|\n",
      "+-------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "+-------+-----------+--------------+\n",
      "|country|country_int|country_onehot|\n",
      "+-------+-----------+--------------+\n",
      "|     KR|        1.0| (2,[1],[1.0])|\n",
      "|     KR|        1.0| (2,[1],[1.0])|\n",
      "|     KR|        1.0| (2,[1],[1.0])|\n",
      "+-------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df5.select(\"country\", \"country_int\", \"country_onehot\").show(3))\n",
    "df5.filter(df5[\"country\"] == \"KR\").select(\"country\", \"country_int\", \"country_onehot\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"probability\", \"country_onehot\"],outputCol=\"corr_features\")\n",
    "df6=assembler.transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|corr_features                                    |\n",
      "+-------------------------------------------------+\n",
      "|[0.9962795654227283,0.003720434577271679,1.0,0.0]|\n",
      "|[0.4722966849601867,0.5277033150398134,1.0,0.0]  |\n",
      "|[0.9430267462702172,0.05697325372978272,1.0,0.0] |\n",
      "+-------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(\"corr_features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pearson(corr_features)=DenseMatrix(4, 4, [1.0, -1.0, 0.0116, -0.0116, -1.0, 1.0, -0.0116, 0.0116, 0.0116, -0.0116, 1.0, -1.0, -0.0116, 0.0116, -1.0, 1.0], False))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답자 국가 간의 Pearson correlation\n",
    "from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df6, 'corr_features', 'pearson').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(featuresCol=\"features\", k=2, tol=0.0001, seed=10)\n",
    "df7 = kmeans.fit(df2).transform(df2) # prediction 생성  # probability 제공되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 클러스터에 속할 확률\n",
    "df7.select(\"prediction\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer : 한 컬럼에 주어진 모든 워드 리스트에 대해 인덱스 벡터 형성 (us, kr) => 0, 1\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_int\")\n",
    "df8 = indexer.fit(df7).transform(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|country|country_int|\n",
      "+-------+-----------+\n",
      "|     US|        0.0|\n",
      "|     US|        0.0|\n",
      "|     US|        0.0|\n",
      "+-------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(\"country\", \"country_int\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCol=\"country_int\", outputCol=\"country_onehot\", dropLast=False)\n",
    "df9 = encoder.transform(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"prediction\", \"country_onehot\"],outputCol=\"corr_features\")\n",
    "df10=assembler.transform(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|corr_features|\n",
      "+-------------+\n",
      "|[1.0,1.0,0.0]|\n",
      "|[0.0,1.0,0.0]|\n",
      "|[0.0,1.0,0.0]|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10.select(\"corr_features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(pearson(corr_features)=DenseMatrix(3, 3, [1.0, 0.0102, -0.0102, 0.0102, 1.0, -1.0, -0.0102, -1.0, 1.0], False))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답자 국가 간의 Pearson correlation\n",
    "from pyspark.ml.stat import Correlation\n",
    "Correlation.corr(df10, 'corr_features', 'pearson').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수행시간(20core, memory 24G) : 18분 11.06초\n"
     ]
    }
   ],
   "source": [
    "spend_time=(time.time() - start_time)\n",
    "print(f'수행시간(20core, memory 24G) : {int(spend_time//60)}분 {spend_time%60:.2f}초')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
