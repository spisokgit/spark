{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"practice - action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce(f)\n",
    "\n",
    "Reduces the elements of this RDD using the specified commutative and associative binary operator. Currently reduces partitions locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(data.reduce(lambda a, b: a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "print(data.reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()\n",
    "\n",
    "Return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()\n",
    "\n",
    "Return the number of elements in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first()\n",
    "\n",
    "Return the first element in this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(num)\n",
    "\n",
    "Take the first num elements of the RDD.\n",
    "\n",
    "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample(withReplacement, num, seed=None)\n",
    "\n",
    "Return a fixed-size sampled subset of this RDD.\n",
    "\n",
    "Note This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered(num, key=None)\n",
    "\n",
    "Get the N elements from an RDD ordered in ascending order or as specified by the optional key function.\n",
    "\n",
    "Note this method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile(path, compressionCodecClass=None)\n",
    "\n",
    "Save this RDD as a text file, using string representations of elements.\n",
    "\n",
    "Parameters\n",
    "* path – path to text file\n",
    "* compressionCodecClass – (None by default) string i.e. “org.apache.hadoop.io.compress.GzipCodec”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()\n",
    "\n",
    "Count the number of elements for each key, and return the result to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach  # driver에 표시되지 않고, worker에서 \n",
    "# 대신 collect() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.glom().collect()  # partition 별로 이중list로  [ [], [], [] ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
